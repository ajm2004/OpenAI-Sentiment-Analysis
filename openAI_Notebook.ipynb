{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI, Consumer based sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- To Be Written --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "from asyncpraw import Reddit\n",
    "from asyncpraw.models import MoreComments\n",
    "\n",
    "# Load Reddit API credentials from environment variables\n",
    "USERNAME = os.getenv('USER')\n",
    "if not USERNAME:\n",
    "    raise ValueError(\"USER environment variable not set\")\n",
    "\n",
    "PASSWORD = os.getenv('PASSWORD')\n",
    "if not PASSWORD:\n",
    "    raise ValueError(\"PASSWORD environment variable not set\")\n",
    "\n",
    "CLIENT_ID = os.getenv('CLIENT_ID')\n",
    "if not CLIENT_ID:\n",
    "    raise ValueError(\"CLIENT_ID environment variable not set\")\n",
    "\n",
    "CLIENT_SECRET = os.getenv('CLIENT_SECRET')\n",
    "if not CLIENT_SECRET:\n",
    "    raise ValueError(\"CLIENT_SECRET environment variable not set\")\n",
    "\n",
    "\n",
    "async def create_reddit_instance():\n",
    "    reddit = Reddit(\n",
    "        client_id=CLIENT_ID,\n",
    "        client_secret=CLIENT_SECRET,\n",
    "        user_agent=\"my user agent\",\n",
    "        username=USERNAME,\n",
    "        password=PASSWORD,\n",
    "    )\n",
    "    \n",
    "    # Enable rate limit handling\n",
    "    reddit.requestor.rate_limit_sleep = True  # Auto-handles rate-limiting\n",
    "    return reddit\n",
    "\n",
    "\n",
    "def load_csv_data(file_path):\n",
    "    \"\"\"\n",
    "    Reads the CSV file and groups the queries by post_id.\n",
    "    Ensures that all query values are strings before joining.\n",
    "    Returns a dictionary mapping post_id -> query string.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert queries to strings and handle NaN values\n",
    "    df[\"query\"] = df[\"query\"].fillna(\"\").astype(str)\n",
    "\n",
    "    # Group by post_id and join unique queries with a semicolon.\n",
    "    grouped = df.groupby(\"post_id\")[\"query\"].apply(lambda x: \";\".join(set(x))).reset_index()\n",
    "    mapping = dict(zip(grouped[\"post_id\"], grouped[\"query\"]))\n",
    "\n",
    "    return mapping\n",
    "\n",
    "\n",
    "async def fetch_post_and_comments(reddit, post_id):\n",
    "    \"\"\"\n",
    "    Fetches the submission and its top 10 first-level comments.\n",
    "    Returns a list of dictionaries containing post and comment details.\n",
    "    In case of an error, returns one dictionary with error details.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    try:\n",
    "        submission = await reddit.submission(id=post_id)  # Await here\n",
    "        \n",
    "        # Load submission details\n",
    "        await submission.load()\n",
    "        \n",
    "        # Fetch top-level comments (limit: 10)\n",
    "        await submission.comments.replace_more(limit=10)\n",
    "        top_comments = submission.comments[:10]\n",
    "\n",
    "        submission_details = {\n",
    "            \"post_id\": submission.id,\n",
    "            \"subreddit\": submission.subreddit.display_name,\n",
    "            \"post_title\": submission.title,\n",
    "            \"post_body\": submission.selftext,\n",
    "            \"number_of_comments\": submission.num_comments,\n",
    "            \"readable_datetime\": datetime.datetime.fromtimestamp(submission.created_utc).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"post_author\": submission.author.name if submission.author else None,\n",
    "        }\n",
    "\n",
    "        if top_comments:\n",
    "            for comment in top_comments:\n",
    "                if isinstance(comment, MoreComments):  # Skip \"load more\" placeholders\n",
    "                    continue\n",
    "                row = submission_details.copy()\n",
    "                row.update({\n",
    "                    \"comment_id\": comment.id,\n",
    "                    \"comment_body\": comment.body,\n",
    "                    \"number_of_upvotes\": comment.score,\n",
    "                    \"comment_author\": comment.author.name if comment.author else None,\n",
    "                })\n",
    "                rows.append(row)\n",
    "        else:\n",
    "            # No comments found; create a row with only post details\n",
    "            row = submission_details.copy()\n",
    "            row.update({\n",
    "                \"comment_id\": None,\n",
    "                \"comment_body\": None,\n",
    "                \"number_of_upvotes\": None,\n",
    "                \"comment_author\": None,\n",
    "            })\n",
    "            rows.append(row)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for post_id {post_id}: {e}\")\n",
    "        # Return an error row where key details (like 'subreddit') are None.\n",
    "        rows.append({\n",
    "            \"post_id\": post_id,\n",
    "            \"subreddit\": None,\n",
    "            \"post_title\": None,\n",
    "            \"post_body\": None,\n",
    "            \"number_of_comments\": None,\n",
    "            \"readable_datetime\": None,\n",
    "            \"post_author\": None,\n",
    "            \"comment_id\": None,\n",
    "            \"comment_body\": None,\n",
    "            \"number_of_upvotes\": None,\n",
    "            \"comment_author\": None,\n",
    "        })\n",
    "    \n",
    "    return rows\n",
    "\n",
    "\n",
    "def save_csv(data, file_name):\n",
    "    \"\"\"Writes the provided data (a list of dictionaries) to a CSV file.\"\"\"\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(file_name, index=False)\n",
    "    print(f\"CSV file saved as {file_name}\")\n",
    "\n",
    "\n",
    "def save_json(data, file_name):\n",
    "    \"\"\"Writes the provided data (a list of dictionaries) to a JSON file.\"\"\"\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4, default=str)\n",
    "    print(f\"JSON file saved as {file_name}\")\n",
    "\n",
    "\n",
    "async def countdown(seconds):\n",
    "    \"\"\"Displays a countdown timer for the given number of seconds.\"\"\"\n",
    "    for remaining in range(seconds, 0, -1):\n",
    "        print(f\"Resuming in {remaining:2d} seconds...\", end=\"\\r\", flush=True)\n",
    "        await asyncio.sleep(1)\n",
    "    print(\"\")  # Move to a new line after countdown\n",
    "\n",
    "\n",
    "async def main():\n",
    "    reddit = await create_reddit_instance()\n",
    "    post_query_mapping = load_csv_data(\"combined_data.csv\")\n",
    "    all_post_ids = list(post_query_mapping.keys())\n",
    "    \n",
    "    # Check for an existing checkpoint (processed post_ids)\n",
    "    checkpoint_file = \"checkpoint.json\"\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            processed_ids = json.load(f)\n",
    "        print(f\"Resuming from checkpoint: {len(processed_ids)} posts already processed.\")\n",
    "    else:\n",
    "        processed_ids = []\n",
    "\n",
    "    # Only process posts that have not yet been processed.\n",
    "    to_process_all = [post_id for post_id in all_post_ids if post_id not in processed_ids]\n",
    "    print(f\"Total posts to process: {len(to_process_all)}\")\n",
    "\n",
    "    # Maintain a retry counter for each post.\n",
    "    retries = {post_id: 0 for post_id in to_process_all}\n",
    "    max_retries = 3  # Maximum attempts per post\n",
    "    batch_size = 30  # Number of posts to process per mini-batch\n",
    "\n",
    "    # Define output file names.\n",
    "    output_csv = \"new_combined_dataset.csv\"\n",
    "    output_json = \"new_combined_dataset.json\"\n",
    "\n",
    "    # If output files already exist, load their data; otherwise, start with an empty list.\n",
    "    if os.path.exists(output_csv):\n",
    "        existing_df = pd.read_csv(output_csv)\n",
    "        output_data = existing_df.to_dict(orient=\"records\")\n",
    "    else:\n",
    "        output_data = []\n",
    "\n",
    "    while to_process_all:\n",
    "        print(f\"\\nStarting a round with {len(to_process_all)} posts to process.\")\n",
    "        next_to_process = []  # To hold posts that need to be retried\n",
    "\n",
    "        # Process posts in mini-batches.\n",
    "        for i in range(0, len(to_process_all), batch_size):\n",
    "            batch_ids = to_process_all[i:i+batch_size]\n",
    "            print(f\"\\nProcessing mini-batch {i // batch_size + 1} \"\n",
    "                  f\"of {((len(to_process_all)-1) // batch_size) + 1} (posts: {batch_ids})\")\n",
    "            \n",
    "            tasks = [fetch_post_and_comments(reddit, post_id) for post_id in batch_ids]\n",
    "            results = await asyncio.gather(*tasks)\n",
    "            \n",
    "            for post_id, rows in zip(batch_ids, results):\n",
    "                # If the first row's \"subreddit\" is None, we assume an error occurred.\n",
    "                if rows and rows[0][\"subreddit\"] is None:\n",
    "                    retries[post_id] += 1\n",
    "                    if retries[post_id] < max_retries:\n",
    "                        print(f\"Error for post_id {post_id}; retrying (attempt {retries[post_id]}/{max_retries}).\")\n",
    "                        next_to_process.append(post_id)\n",
    "                    else:\n",
    "                        print(f\"Error for post_id {post_id} after {max_retries} attempts; logging error.\")\n",
    "                        for row in rows:\n",
    "                            row[\"query\"] = post_query_mapping.get(post_id, \"\")\n",
    "                        output_data.extend(rows)\n",
    "                        processed_ids.append(post_id)\n",
    "                else:\n",
    "                    # Successful result.\n",
    "                    for row in rows:\n",
    "                        row[\"query\"] = post_query_mapping.get(post_id, \"\")\n",
    "                    output_data.extend(rows)\n",
    "                    processed_ids.append(post_id)\n",
    "            \n",
    "            # Incrementally save output after each mini-batch.\n",
    "            save_csv(output_data, output_csv)\n",
    "            save_json(output_data, output_json)\n",
    "            # Update the checkpoint.\n",
    "            with open(checkpoint_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(processed_ids, f, indent=4)\n",
    "            \n",
    "            print(\"Mini-batch processed. Waiting 60 seconds before the next mini-batch...\")\n",
    "            await countdown(60)\n",
    "\n",
    "        if next_to_process:\n",
    "            print(f\"\\nRetrying {len(next_to_process)} posts in the next round...\\n\")\n",
    "            to_process_all = next_to_process\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print(\"All posts processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_34020\\1641351664.py:47: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total posts to process: 9575\n",
      "\n",
      "Starting a round with 9575 posts to process.\n",
      "\n",
      "Processing mini-batch 1 of 320 (posts: [' the next era is the niche content producer who produces high quality video content', '1002dom', '1007cpq', '100ayoe', '100ye6s', '101chgd', '101melg', '101ms83', '101o6zx', '101p00n', '102ci8x', '102jcse', '102l28b', '102lbp8', '102lrwi', '102xqim', '1030pti', '1030xji', '1031mz4', '1031yi2', '10346f5', '1035gzm', '103ahhi', '103gran', '103qc9j', '103vj6v', '103w7m4', '103wsie', '103yg7r', '1041tuw'])\n",
      "Error fetching data for post_id  the next era is the niche content producer who produces high quality video content: received 404 HTTP response\n",
      "Error fetching data for post_id 1030xji: received 429 HTTP response\n",
      "Error for post_id  the next era is the niche content producer who produces high quality video content; retrying (attempt 1/3).\n",
      "Error for post_id 1030xji; retrying (attempt 1/3).\n",
      "CSV file saved as new_combined_dataset.csv\n",
      "JSON file saved as new_combined_dataset.json\n",
      "Mini-batch processed. Waiting 60 seconds before the next mini-batch...\n",
      "Resuming in  1 seconds...\n",
      "\n",
      "Processing mini-batch 2 of 320 (posts: ['10455yl', '1048w07', '104afqz', '104cinw', '104dst5', '104lm2z', '104mdxr', '104w4z9', '104xpc8', '105afmq', '105l3t4', '105moet', '105pib0', '10693m7', '106sd1z', '1075q6q', '1077mrb', '107fjv4', '107hois', '107iyyc', '107qi1v', '107tpuz', '107zfxk', '108360k', '108bjuh', '108fejo', '108o5dd', '108rsgi', '108ti08', '108z4cu'])\n",
      "CSV file saved as new_combined_dataset.csv\n",
      "JSON file saved as new_combined_dataset.json\n",
      "Mini-batch processed. Waiting 60 seconds before the next mini-batch...\n",
      "Resuming in  1 seconds...\n",
      "\n",
      "Processing mini-batch 3 of 320 (posts: ['10954s1', '10976w9', '109a8zh', '109gwvf', '109p6sg', '109tdfw', '109tdpy', '109usd4', '109xy54', '109y3up', '10a2lnv', '10a48tb', '10ajtyq', '10aysln', '10b2qgf', '10b45lm', '10blgtr', '10bo3pn', '10bpbn7', '10bpgkk', '10bqgcq', '10c6g6p', '10cbome', '10cfa5i', '10cofea', '10cpkch', '10d4ufr', '10d7f5y', '10dgzhm', '10dmdu2'])\n",
      "CSV file saved as new_combined_dataset.csv\n",
      "JSON file saved as new_combined_dataset.json\n",
      "Mini-batch processed. Waiting 60 seconds before the next mini-batch...\n",
      "Resuming in  1 seconds...\n",
      "\n",
      "Processing mini-batch 4 of 320 (posts: ['10dsx46', '10e1qo0', '10e359c', '10e4qyl', '10ecqkk', '10ed1tc', '10edrxq', '10ega74', '10ekcxy', '10en5d9', '10eob1h', '10f61vq', '10f9324', '10fexml', '10fj3em', '10fratj', '10fysjh', '10gc2rp', '10ggoow', '10gowdd', '10gyd9v', '10gzpn1', '10h3983', '10hd0vu', '10heb2k', '10hf2s5', '10hfrbu', '10hjb5d', '10hkij4', '10hppwl'])\n",
      "Error fetching data for post_id 10hf2s5: received 429 HTTP response\n",
      "Error fetching data for post_id 10gowdd: received 429 HTTP response\n",
      "Error fetching data for post_id 10f61vq: received 429 HTTP response\n",
      "Error for post_id 10f61vq; retrying (attempt 1/3).\n",
      "Error for post_id 10gowdd; retrying (attempt 1/3).\n",
      "Error for post_id 10hf2s5; retrying (attempt 1/3).\n",
      "CSV file saved as new_combined_dataset.csv\n",
      "JSON file saved as new_combined_dataset.json\n",
      "Mini-batch processed. Waiting 60 seconds before the next mini-batch...\n",
      "Resuming in  1 seconds...\n",
      "\n",
      "Processing mini-batch 5 of 320 (posts: ['10hzjyv', '10i0w61', '10i1dsp', '10icybh', '10ilc0m', '10iot9i', '10iuoz0', '10j3gzy', '10j6grl', '10jcrb8', '10ji6o3', '10jmlwt', '10jq6yr', '10jvumq', '10jw3f7', '10jyqyh', '10k1j4c', '10k2oyh', '10k5p66', '10kly2o', '10kqpf9', '10kx69n', '10kxhkp', '10kyrdd', '10l0p9t', '10l1qaj', '10l5bmq', '10l66hl', '10l8zi5', '10lfg6b'])\n",
      "CSV file saved as new_combined_dataset.csv\n",
      "JSON file saved as new_combined_dataset.json\n",
      "Mini-batch processed. Waiting 60 seconds before the next mini-batch...\n",
      "Resuming in  1 seconds...\n",
      "\n",
      "Processing mini-batch 6 of 320 (posts: ['10lkoa7', '10lmnxj', '10lnqbl', '10losmp', '10loyl3', '10lvfnt', '10lz2ya', '10m4icf', '10mhzhk', '10mhzt8', '10mo873', '10mv4ol', '10n53mg', '10n5811', '10n7msi', '10ne7tz', '10nifag', '10o4jte', '10o5yf0', '10obbe6', '10ocwa2', '10oy03g', '10ozflx', '10p0i1d', '10p0scg', '10p1kwi', '10pb1y7', '10pfbyv', '10pj2n5', '10pnm76'])\n",
      "CSV file saved as new_combined_dataset.csv\n",
      "JSON file saved as new_combined_dataset.json\n",
      "Mini-batch processed. Waiting 60 seconds before the next mini-batch...\n",
      "Resuming in  1 seconds...\n",
      "\n",
      "Processing mini-batch 7 of 320 (posts: ['10pof51', '10pxbj3', '10q0bxo', '10q1r0b', '10q678f', '10q6ptf', '10qdfpk', '10qgi7c', '10r263w', '10r26i1', '10r2e7g', '10r47g9', '10r7amb', '10r7xxo', '10rjfk0', '10rk5o3', '10rniwf', '10ro9gi', '10rtwc5', '10s7r7e', '10s9tpu', '10shod4', '10sq6wx', '10sw51o', '10swbzq', '10t65ds', '10tg6o0', '10tiy7c', '10tkibu', '10tmjp4'])\n",
      "Error fetching data for post_id 10tmjp4: received 429 HTTP response\n",
      "Error fetching data for post_id 10ro9gi: received 429 HTTP response\n",
      "Error fetching data for post_id 10sq6wx: received 429 HTTP response\n",
      "Error fetching data for post_id 10tkibu: received 429 HTTP response\n",
      "Error for post_id 10ro9gi; retrying (attempt 1/3).\n",
      "Error for post_id 10sq6wx; retrying (attempt 1/3).\n",
      "Error for post_id 10tkibu; retrying (attempt 1/3).\n",
      "Error for post_id 10tmjp4; retrying (attempt 1/3).\n",
      "CSV file saved as new_combined_dataset.csv\n",
      "JSON file saved as new_combined_dataset.json\n",
      "Mini-batch processed. Waiting 60 seconds before the next mini-batch...\n",
      "Resuming in  1 seconds...\n",
      "\n",
      "Processing mini-batch 8 of 320 (posts: ['10to1fg', '10u29ak', '10u9a2e', '10uaj02', '10uct4o', '10ugiqb', '10uobs1', '10v1z5b', '10v74r0', '10vf4uf', '10vf8ke', '10vfala', '10vm6fh', '10vmssr', '10vs0j9', '10vus33', '10vwck5', '10w2jfr', '10w4aus', '10w6i31', '10w952k', '10wblvi', '10we2vn', '10wh2gx', '10wii9u', '10wqg7w', '10ws1pi', '10wwlpm', '10x73u9', '10xajfd'])\n",
      "Error fetching data for post_id 10uct4o: received 429 HTTP response\n",
      "Error for post_id 10uct4o; retrying (attempt 1/3).\n",
      "CSV file saved as new_combined_dataset.csv\n",
      "JSON file saved as new_combined_dataset.json\n",
      "Mini-batch processed. Waiting 60 seconds before the next mini-batch...\n",
      "Resuming in  1 seconds...\n",
      "\n",
      "Processing mini-batch 9 of 320 (posts: ['10xjda1', '10xonlr', '10xstir', '10xukik', '10xyqly', '10xz6o7', '10yd4rl', '10ye3a8', '10yf7n5', '10yj8xj', '10yt20s', '10ywuv9', '10yzfyd', '10z0m1n', '10z11e2', '10zbsdw', '10zcbqn', '10ze84u', '10zfa13', '10zk56z', '10zt1x5', '10zte58', '10zve0u', '10zxpzy', '1100gvy', '1102ht7', '1103r9p', '11057v5', '1108odw', '110gan5'])\n",
      "Error fetching data for post_id 1103r9p: received 429 HTTP response\n",
      "Error fetching data for post_id 10xjda1: received 429 HTTP response\n",
      "Error fetching data for post_id 10ze84u: received 429 HTTP response\n",
      "Error for post_id 10xjda1; retrying (attempt 1/3).\n",
      "Error for post_id 10ze84u; retrying (attempt 1/3).\n",
      "Error for post_id 1103r9p; retrying (attempt 1/3).\n",
      "CSV file saved as new_combined_dataset.csv\n",
      "JSON file saved as new_combined_dataset.json\n",
      "Mini-batch processed. Waiting 60 seconds before the next mini-batch...\n",
      "Resuming in  1 seconds...\n",
      "\n",
      "Processing mini-batch 10 of 320 (posts: ['110ih6y', '110r2j4', '110yfrw', '11116ro', '1111c2d', '1112myn', '1113joc', '11147xr', '1115bd7', '111j4qc', '1123s6c', '1124hba', '1125jiq', '1125twm', '1126jl6', '1126lf3', '1127vz1', '112mdze', '112ni25', '112ouyi', '112uof7', '112yytk', '113016w', '113e8qa', '113j0j5', '113k223', '113kh0f', '113p2jn', '113rn3x', '113t45q'])\n",
      "CSV file saved as new_combined_dataset.csv\n",
      "JSON file saved as new_combined_dataset.json\n",
      "Mini-batch processed. Waiting 60 seconds before the next mini-batch...\n",
      "Resuming in  1 seconds...\n",
      "\n",
      "Processing mini-batch 11 of 320 (posts: ['113u29k', '113vhh1', '113x3au', '1143qws', '1144724', '11463wh', '1146zfh', '114a4ql', '114f794', '114mzwa', '114sfyv', '114y82i', '114yknl', '1152az5', '11561xe', '115cjmh', '115d404', '115g3yd', '115lgc7', '115nbbf', '115tzsn', '115ysbo', '1161bpr', '1162kqz', '1164a4d', '1169fhw', '116fol5', '116pify', '116rle2', '116sfcd'])\n",
      "CSV file saved as new_combined_dataset.csv\n",
      "JSON file saved as new_combined_dataset.json\n",
      "Mini-batch processed. Waiting 60 seconds before the next mini-batch...\n",
      "Resuming in  1 seconds...\n",
      "\n",
      "Processing mini-batch 12 of 320 (posts: ['116xg8b', '1171o2h', '117c9bh', '117hc4t', '117vnpi', '117zh6h', '1182tg4', '1188qdt', '1188uia', '118e80f', '118hk9v', '118iyl9', '118mmxs', '118p387', '118r19a', '118rq37', '118sopc', '118ss0x', '118tcbc', '118w8a8', '118zu8r', '1190i25', '1194fxm', '1195fpd', '119a2nl', '119c3k7', '119grrx', '11a0lxu', '11a2gnb', '11a7dax'])\n",
      "CSV file saved as new_combined_dataset.csv\n",
      "JSON file saved as new_combined_dataset.json\n",
      "Mini-batch processed. Waiting 60 seconds before the next mini-batch...\n",
      "Resuming in 12 seconds...\r"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def run_async():\n",
    "    await main()\n",
    "\n",
    "await run_async()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline (Part - C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
