{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI, Consumer based sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- To Be Written --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "from asyncpraw import Reddit\n",
    "from asyncpraw.models import MoreComments\n",
    "\n",
    "\n",
    "# Load Reddit API credentials from environment variables\n",
    "USERNAME = os.getenv('USER')\n",
    "if not USERNAME:\n",
    "    raise ValueError(\"USER environment variable not set\")\n",
    "\n",
    "PASSWORD = os.getenv('PASSWORD')\n",
    "if not PASSWORD:\n",
    "    raise ValueError(\"PASSWORD environment variable not set\")\n",
    "\n",
    "CLIENT_ID = os.getenv('CLIENT_ID')\n",
    "if not CLIENT_ID:\n",
    "    raise ValueError(\"CLIENT_ID environment variable not set\")\n",
    "\n",
    "CLIENT_SECRET = os.getenv('CLIENT_SECRET')\n",
    "if not CLIENT_SECRET:\n",
    "    raise ValueError(\"CLIENT_SECRET environment variable not set\")\n",
    "\n",
    "\n",
    "async def create_reddit_instance():\n",
    "    reddit = Reddit(\n",
    "        client_id=CLIENT_ID,\n",
    "        client_secret=CLIENT_SECRET,\n",
    "        user_agent=\"my user agent\",\n",
    "        username=USERNAME,\n",
    "        password=PASSWORD,\n",
    "    )\n",
    "    \n",
    "    # Enable rate limit handling\n",
    "    reddit.requestor.rate_limit_sleep = True  # ✅ Auto-handles rate-limiting\n",
    "    return reddit\n",
    "\n",
    "\n",
    "def load_csv_data(file_path):\n",
    "    \"\"\"\n",
    "    Reads the CSV file and groups the queries by post_id.\n",
    "    Ensures that all query values are strings before joining.\n",
    "    Returns a dictionary mapping post_id -> query string.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Convert queries to strings and handle NaN values\n",
    "    df[\"query\"] = df[\"query\"].fillna(\"\").astype(str)\n",
    "\n",
    "    # Group by post_id and join unique queries with a semicolon.\n",
    "    grouped = df.groupby(\"post_id\")[\"query\"].apply(lambda x: \";\".join(set(x))).reset_index()\n",
    "    mapping = dict(zip(grouped[\"post_id\"], grouped[\"query\"]))\n",
    "\n",
    "    return mapping\n",
    "\n",
    "async def fetch_post_and_comments(reddit, post_id):\n",
    "    \"\"\"\n",
    "    Fetches the submission and its top 10 first-level comments.\n",
    "    Returns a list of dictionaries containing post and comment details.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    try:\n",
    "        submission = await reddit.submission(id=post_id)  # ✅ Await here\n",
    "        \n",
    "        # Load submission details\n",
    "        await submission.load()\n",
    "        \n",
    "        # Fetch top-level comments (limit: 10)\n",
    "        await submission.comments.replace_more(limit=10)\n",
    "        top_comments = submission.comments[:10]\n",
    "\n",
    "        submission_details = {\n",
    "            \"post_id\": submission.id,\n",
    "            \"subreddit\": submission.subreddit.display_name,\n",
    "            \"post_title\": submission.title,\n",
    "            \"post_body\": submission.selftext,\n",
    "            \"number_of_comments\": submission.num_comments,\n",
    "            \"readable_datetime\": datetime.datetime.fromtimestamp(submission.created_utc).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"post_author\": submission.author.name if submission.author else None,\n",
    "        }\n",
    "\n",
    "        if top_comments:\n",
    "            for comment in top_comments:\n",
    "                if isinstance(comment, MoreComments):  # Skip \"load more\" placeholders\n",
    "                    continue\n",
    "                row = submission_details.copy()\n",
    "                row.update({\n",
    "                    \"comment_id\": comment.id,\n",
    "                    \"comment_body\": comment.body,\n",
    "                    \"number_of_upvotes\": comment.score,\n",
    "                    \"comment_author\": comment.author.name if comment.author else None,\n",
    "                })\n",
    "                rows.append(row)\n",
    "        else:\n",
    "            # No comments found; create a row with only post details\n",
    "            row = submission_details.copy()\n",
    "            row.update({\n",
    "                \"comment_id\": None,\n",
    "                \"comment_body\": None,\n",
    "                \"number_of_upvotes\": None,\n",
    "                \"comment_author\": None,\n",
    "            })\n",
    "            rows.append(row)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for post_id {post_id}: {e}\")\n",
    "        rows.append({\n",
    "            \"post_id\": post_id,\n",
    "            \"subreddit\": None,\n",
    "            \"post_title\": None,\n",
    "            \"post_body\": None,\n",
    "            \"number_of_comments\": None,\n",
    "            \"readable_datetime\": None,\n",
    "            \"post_author\": None,\n",
    "            \"comment_id\": None,\n",
    "            \"comment_body\": None,\n",
    "            \"number_of_upvotes\": None,\n",
    "            \"comment_author\": None,\n",
    "        })\n",
    "    \n",
    "    return rows\n",
    "\n",
    "\n",
    "def write_csv(data, file_name):\n",
    "    \"\"\"\n",
    "    Writes the provided data (a list of dictionaries) to a CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(file_name, index=False)\n",
    "    print(f\"CSV file saved as {file_name}\")\n",
    "\n",
    "def write_json(data, file_name):\n",
    "    \"\"\"\n",
    "    Writes the provided data (a list of dictionaries) to a JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4, default=str)\n",
    "    print(f\"JSON file saved as {file_name}\")\n",
    "\n",
    "async def main():\n",
    "    reddit = await create_reddit_instance()\n",
    "    post_query_mapping = load_csv_data(\"combined_data.csv\")\n",
    "    post_ids = list(post_query_mapping.keys())\n",
    "    output_data = []\n",
    "    \n",
    "    batch_size = 10  # Process 10 posts at a time; adjust based on your needs.\n",
    "    \n",
    "    for i in range(0, len(post_ids), batch_size):\n",
    "        batch_ids = post_ids[i:i+batch_size]\n",
    "        tasks = [fetch_post_and_comments(reddit, post_id) for post_id in batch_ids]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # Process the results for this batch\n",
    "        for post_id, rows in zip(batch_ids, results):\n",
    "            query = post_query_mapping[post_id]\n",
    "            for row in rows:\n",
    "                row[\"query\"] = query\n",
    "                output_data.append(row)\n",
    "                \n",
    "        print(f\"Processed batch {i // batch_size + 1} / {((len(post_ids) - 1) // batch_size) + 1}. Sleeping for 60 seconds...\")\n",
    "        await asyncio.sleep(60)  # Sleep for 60 seconds between batches\n",
    "\n",
    "    write_csv(output_data, \"new_combined_dataset.csv\")\n",
    "    write_json(output_data, \"new_combined_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_25952\\2941016397.py:48: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for post_id 101o6zx: received 429 HTTP response\n",
      "Error fetching data for post_id 101ms83: received 429 HTTP response\n",
      "Error fetching data for post_id 101p00n: received 429 HTTP response\n",
      "Error fetching data for post_id 100ye6s: received 429 HTTP response\n",
      "Error fetching data for post_id 101chgd: received 429 HTTP response\n",
      "Error fetching data for post_id  the next era is the niche content producer who produces high quality video content: received 429 HTTP response\n",
      "Error fetching data for post_id 100ayoe: received 429 HTTP response\n",
      "Error fetching data for post_id 101melg: received 429 HTTP response\n",
      "Error fetching data for post_id 1007cpq: received 429 HTTP response\n",
      "Error fetching data for post_id 1002dom: received 429 HTTP response\n",
      "Processed batch 1 / 958. Sleeping for 60 seconds...\n",
      "Error fetching data for post_id 102l28b: received 429 HTTP response\n",
      "Error fetching data for post_id 102ci8x: received 429 HTTP response\n",
      "Error fetching data for post_id 102jcse: received 429 HTTP response\n",
      "Error fetching data for post_id 102xqim: received 429 HTTP response\n",
      "Error fetching data for post_id 1030xji: received 429 HTTP response\n",
      "Error fetching data for post_id 1031mz4: received 429 HTTP response\n",
      "Error fetching data for post_id 1030pti: received 429 HTTP response\n",
      "Error fetching data for post_id 102lrwi: received 429 HTTP response\n",
      "Error fetching data for post_id 1031yi2: received 429 HTTP response\n",
      "Error fetching data for post_id 102lbp8: received 429 HTTP response\n",
      "Processed batch 2 / 958. Sleeping for 60 seconds...\n",
      "Error fetching data for post_id 103gran: received 429 HTTP response\n",
      "Error fetching data for post_id 103w7m4: received 429 HTTP response\n",
      "Error fetching data for post_id 1035gzm: received 429 HTTP response\n",
      "Error fetching data for post_id 103ahhi: received 429 HTTP response\n",
      "Error fetching data for post_id 103vj6v: received 429 HTTP response\n",
      "Error fetching data for post_id 1041tuw: received 429 HTTP response\n",
      "Error fetching data for post_id 10346f5: received 429 HTTP response\n",
      "Error fetching data for post_id 103wsie: received 429 HTTP response\n",
      "Error fetching data for post_id 103yg7r: received 429 HTTP response\n",
      "Error fetching data for post_id 103qc9j: received 429 HTTP response\n",
      "Processed batch 3 / 958. Sleeping for 60 seconds...\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def run_async():\n",
    "    await main()\n",
    "\n",
    "await run_async()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline (Part - C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
