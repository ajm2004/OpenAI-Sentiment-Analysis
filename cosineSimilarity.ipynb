{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install contractions\n",
    "# %pip install textblob\n",
    "# %pip install nltk\n",
    "# %pip install scikit-learn\n",
    "# %pip install transformers\n",
    "# %pip install emoji\n",
    "# %pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction import text\n",
    "import string\n",
    "import contractions\n",
    "import emoji\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11096\\1927029670.py:1: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"combined_data_chatgpt_openAi.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>comments</th>\n",
       "      <th>date_time</th>\n",
       "      <th>author</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1hr4hc6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Weekly Self-Promotional Mega Thread 49, 01.01....</td>\n",
       "      <td>All the self-promotional posts about your AI p...</td>\n",
       "      <td>ChatGPT</td>\n",
       "      <td>23</td>\n",
       "      <td>118</td>\n",
       "      <td>2025-01-01 14:58:15</td>\n",
       "      <td>pirate_jack_sparrow_</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ggixzy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AMA with OpenAI’s Sam Altman, Kevin Weil, Srin...</td>\n",
       "      <td>Consider this AMA our Reddit launch.\\n\\nAsk us...</td>\n",
       "      <td>ChatGPT</td>\n",
       "      <td>4000</td>\n",
       "      <td>4705</td>\n",
       "      <td>2024-10-31 16:40:38</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1id5l47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OpenAI Pleads That It Can’t Make Money Without...</td>\n",
       "      <td># \"It would be impossible to train today’s lea...</td>\n",
       "      <td>ChatGPT</td>\n",
       "      <td>1135</td>\n",
       "      <td>182</td>\n",
       "      <td>2025-01-29 21:45:58</td>\n",
       "      <td>faustoc5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1icyjx6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remember When OpenAI Threatened Your Job? A Fr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ChatGPT</td>\n",
       "      <td>1222</td>\n",
       "      <td>131</td>\n",
       "      <td>2025-01-29 16:59:46</td>\n",
       "      <td>Dexter01010</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1icvvjq</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"My AI just absolutely roasted me and I'm ques...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ChatGPT</td>\n",
       "      <td>1362</td>\n",
       "      <td>159</td>\n",
       "      <td>2025-01-29 15:08:12</td>\n",
       "      <td>maruhadapurpurine</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id comment_id                                              title  \\\n",
       "0  1hr4hc6        NaN  Weekly Self-Promotional Mega Thread 49, 01.01....   \n",
       "1  1ggixzy        NaN  AMA with OpenAI’s Sam Altman, Kevin Weil, Srin...   \n",
       "2  1id5l47        NaN  OpenAI Pleads That It Can’t Make Money Without...   \n",
       "3  1icyjx6        NaN  Remember When OpenAI Threatened Your Job? A Fr...   \n",
       "4  1icvvjq        NaN  \"My AI just absolutely roasted me and I'm ques...   \n",
       "\n",
       "                                                body subreddit upvotes  \\\n",
       "0  All the self-promotional posts about your AI p...   ChatGPT      23   \n",
       "1  Consider this AMA our Reddit launch.\\n\\nAsk us...   ChatGPT    4000   \n",
       "2  # \"It would be impossible to train today’s lea...   ChatGPT    1135   \n",
       "3                                                NaN   ChatGPT    1222   \n",
       "4                                                NaN   ChatGPT    1362   \n",
       "\n",
       "  comments            date_time                author query  \n",
       "0      118  2025-01-01 14:58:15  pirate_jack_sparrow_   NaN  \n",
       "1     4705  2024-10-31 16:40:38                OpenAI   NaN  \n",
       "2      182  2025-01-29 21:45:58              faustoc5   NaN  \n",
       "3      131  2025-01-29 16:59:46           Dexter01010   NaN  \n",
       "4      159  2025-01-29 15:08:12     maruhadapurpurine   NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"combined_data_chatgpt_openAi.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Weekly Self-Promotional Mega Thread 49, 01.01....\n",
       "1    AMA with OpenAI’s Sam Altman, Kevin Weil, Srin...\n",
       "2    OpenAI Pleads That It Can’t Make Money Without...\n",
       "3    Remember When OpenAI Threatened Your Job? A Fr...\n",
       "4    \"My AI just absolutely roasted me and I'm ques...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"text\"] = data.apply(lambda row: f\"{row['title']} {row['body']}\" if pd.isna(row['comment_id']) else row['body'], axis=1)\n",
    "data[\"text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing blank rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>comments</th>\n",
       "      <th>date_time</th>\n",
       "      <th>author</th>\n",
       "      <th>query</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1hr4hc6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Weekly Self-Promotional Mega Thread 49, 01.01....</td>\n",
       "      <td>All the self-promotional posts about your AI p...</td>\n",
       "      <td>ChatGPT</td>\n",
       "      <td>23</td>\n",
       "      <td>118</td>\n",
       "      <td>2025-01-01 14:58:15</td>\n",
       "      <td>pirate_jack_sparrow_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Weekly Self-Promotional Mega Thread 49, 01.01....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ggixzy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AMA with OpenAI’s Sam Altman, Kevin Weil, Srin...</td>\n",
       "      <td>Consider this AMA our Reddit launch.\\n\\nAsk us...</td>\n",
       "      <td>ChatGPT</td>\n",
       "      <td>4000</td>\n",
       "      <td>4705</td>\n",
       "      <td>2024-10-31 16:40:38</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AMA with OpenAI’s Sam Altman, Kevin Weil, Srin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1id5l47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OpenAI Pleads That It Can’t Make Money Without...</td>\n",
       "      <td># \"It would be impossible to train today’s lea...</td>\n",
       "      <td>ChatGPT</td>\n",
       "      <td>1135</td>\n",
       "      <td>182</td>\n",
       "      <td>2025-01-29 21:45:58</td>\n",
       "      <td>faustoc5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OpenAI Pleads That It Can’t Make Money Without...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1icyjx6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remember When OpenAI Threatened Your Job? A Fr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ChatGPT</td>\n",
       "      <td>1222</td>\n",
       "      <td>131</td>\n",
       "      <td>2025-01-29 16:59:46</td>\n",
       "      <td>Dexter01010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remember When OpenAI Threatened Your Job? A Fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1icvvjq</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"My AI just absolutely roasted me and I'm ques...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ChatGPT</td>\n",
       "      <td>1362</td>\n",
       "      <td>159</td>\n",
       "      <td>2025-01-29 15:08:12</td>\n",
       "      <td>maruhadapurpurine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"My AI just absolutely roasted me and I'm ques...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id comment_id                                              title  \\\n",
       "0  1hr4hc6        NaN  Weekly Self-Promotional Mega Thread 49, 01.01....   \n",
       "1  1ggixzy        NaN  AMA with OpenAI’s Sam Altman, Kevin Weil, Srin...   \n",
       "2  1id5l47        NaN  OpenAI Pleads That It Can’t Make Money Without...   \n",
       "3  1icyjx6        NaN  Remember When OpenAI Threatened Your Job? A Fr...   \n",
       "4  1icvvjq        NaN  \"My AI just absolutely roasted me and I'm ques...   \n",
       "\n",
       "                                                body subreddit upvotes  \\\n",
       "0  All the self-promotional posts about your AI p...   ChatGPT      23   \n",
       "1  Consider this AMA our Reddit launch.\\n\\nAsk us...   ChatGPT    4000   \n",
       "2  # \"It would be impossible to train today’s lea...   ChatGPT    1135   \n",
       "3                                                NaN   ChatGPT    1222   \n",
       "4                                                NaN   ChatGPT    1362   \n",
       "\n",
       "  comments            date_time                author query  \\\n",
       "0      118  2025-01-01 14:58:15  pirate_jack_sparrow_   NaN   \n",
       "1     4705  2024-10-31 16:40:38                OpenAI   NaN   \n",
       "2      182  2025-01-29 21:45:58              faustoc5   NaN   \n",
       "3      131  2025-01-29 16:59:46           Dexter01010   NaN   \n",
       "4      159  2025-01-29 15:08:12     maruhadapurpurine   NaN   \n",
       "\n",
       "                                                text  \n",
       "0  Weekly Self-Promotional Mega Thread 49, 01.01....  \n",
       "1  AMA with OpenAI’s Sam Altman, Kevin Weil, Srin...  \n",
       "2  OpenAI Pleads That It Can’t Make Money Without...  \n",
       "3  Remember When OpenAI Threatened Your Job? A Fr...  \n",
       "4  \"My AI just absolutely roasted me and I'm ques...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Blank Rows\n",
    "data = data.dropna(subset=[\"text\"])\n",
    "data = data.reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting texts to lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    weekly self-promotional mega thread 49, 01.01....\n",
       "1    ama with openai’s sam altman, kevin weil, srin...\n",
       "2    openai pleads that it can’t make money without...\n",
       "3    remember when openai threatened your job? a fr...\n",
       "4    \"my ai just absolutely roasted me and i'm ques...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lowercase\n",
    "def lower(text):\n",
    "  return text.lower()\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"text\"].apply(lower)\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing r/, usernames, new line indicators, and links from texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    weekly self-promotional mega thread 49, 01.01....\n",
       "1    ama with openai’s sam altman, kevin weil, srin...\n",
       "2    openai pleads that it can’t make money without...\n",
       "3    remember when openai threatened your job? a fr...\n",
       "4    \"my ai just absolutely roasted me and i'm ques...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove unnecessary tokens\n",
    "def remove_links(text):\n",
    "  return re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "def remove_user_mentions(text):\n",
    "    return re.sub(r'u/\\S+', '', text)\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].str.replace('r/', '', regex=False)\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].str.replace(\"\\n\\n\", ' ', regex=False)\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(remove_links)\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(remove_user_mentions)\n",
    "\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing spelling errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fix spelling\n",
    "# def correct_spelling(text):\n",
    "#     return str(TextBlob(text).correct())\n",
    "\n",
    "# data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(correct_spelling)\n",
    "# data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding contractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    weekly self-promotional mega thread 49, 01.01....\n",
       "1    ama with openai’s sam altman, kevin weil, srin...\n",
       "2    openai pleads that it cannot make money withou...\n",
       "3    remember when openai threatened your job? a fr...\n",
       "4    \"my ai just absolutely roasted me and i am que...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expand contractions\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(expand_contractions)\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    weekly self-promotional mega thread 49, 01.01....\n",
       "1    ama openai’s sam altman, kevin weil, srinivas ...\n",
       "2    openai pleads make money using copyrighted mat...\n",
       "3    remember openai threatened job? free ai just r...\n",
       "4    \"my ai just absolutely roasted questioning rea...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "stopwords = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(\n",
    "    lambda text: ' '.join([word for word in text.split() if word.lower() not in stopwords])\n",
    ")\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing punctuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    weekly self promotional mega thread 49 01 01 2...\n",
       "1    ama openai’s sam altman kevin weil srinivas na...\n",
       "2    openai pleads make money using copyrighted mat...\n",
       "3    remember openai threatened job free ai just re...\n",
       "4    my ai just absolutely roasted questioning real...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuations\n",
    "def remove_punctuations(text):\n",
    "    text = re.sub(r'[-]', ' ', text)\n",
    "    text = re.sub(r'(\\S)[' + re.escape(string.punctuation) + r'](\\S)', r'\\1 \\2', text)\n",
    "    return text\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(remove_punctuations)\n",
    "\n",
    "pattern_punctuations = r'[' + string.punctuation + r']'\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].str.replace(pattern_punctuations, '', regex=True)\n",
    "\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting emojis to their descriptive names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    weekly self promotional mega thread 49 01 01 2...\n",
       "1    ama openai’s sam altman kevin weil srinivas na...\n",
       "2    openai pleads make money using copyrighted mat...\n",
       "3    remember openai threatened job free ai just re...\n",
       "4    my ai just absolutely roasted questioning real...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_emojis(text):\n",
    "   return emoji.demojize(text)\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(convert_emojis)\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ca', 'nt'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  0 0  0 001  0 002  0 01  0 02  0 03  0 03 1000  0 05  0 06  ...  ⣷  ⣹  \\\n",
      "0  0    0      0      0     0     0     0          0     0     0  ...  0  0   \n",
      "1  0    0      0      0     0     0     0          0     0     0  ...  0  0   \n",
      "2  0    0      0      0     0     0     0          0     0     0  ...  0  0   \n",
      "3  0    0      0      0     0     0     0          0     0     0  ...  0  0   \n",
      "4  0    0      0      0     0     0     0          0     0     0  ...  0  0   \n",
      "\n",
      "   ⣼  ⣽  ⣾  ⣿  ツ  人權  諾貝爾和平獎  ￼  \n",
      "0  0  0  0  0  0   0       0  0  \n",
      "1  0  0  0  0  0   0       0  0  \n",
      "2  0  0  0  0  0   0       0  0  \n",
      "3  0  0  0  0  0   0       0  0  \n",
      "4  0  0  0  0  0   0       0  0  \n",
      "\n",
      "[5 rows x 25000 columns]\n"
     ]
    }
   ],
   "source": [
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def custom_tokenizer(text):\n",
    "#     doc = nlp(text)\n",
    "#     tokens = []\n",
    "#     for ent in doc.ents:\n",
    "#         tokens.append(ent.text)\n",
    "\n",
    "#     non_entity_tokens = [token.text for token in doc if not token.ent_type_ and not token.is_punct and not token.is_space]\n",
    "#     tokens.extend(non_entity_tokens)\n",
    "#     return tokens\n",
    "\n",
    "# vectorizer = TfidfVectorizer(\n",
    "#     tokenizer=custom_tokenizer,\n",
    "#     use_idf=False\n",
    "# )\n",
    "\n",
    "# doc_vectors = vectorizer.fit_transform(data[\"Cleaned Text\"])\n",
    "\n",
    "# feature_names = vectorizer.get_feature_names_out()\n",
    "# # Instead of converting to dense matrix, we can directly use the sparse matrix\n",
    "# tfidf_data = pd.DataFrame(doc_vectors.toarray(), columns=feature_names)\n",
    "# tfidf_data\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [ent.text for ent in doc.ents]  # Named Entities\n",
    "    non_entity_tokens = [token.text for token in doc if not token.ent_type_ and not token.is_punct and not token.is_space]\n",
    "    tokens.extend(non_entity_tokens)\n",
    "    return tokens\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=custom_tokenizer,\n",
    "    use_idf=False,\n",
    "    stop_words=\"english\",  # Remove common words\n",
    "    max_features=25000,     # Reduce vocabulary size\n",
    "    min_df=5               # Ignore words appearing in <5 docs\n",
    ")\n",
    "\n",
    "doc_vectors = vectorizer.fit_transform(data[\"Cleaned Text\"]).astype(\"float32\")\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Keep it sparse to save memory\n",
    "tfidf_data = pd.DataFrame.sparse.from_spmatrix(doc_vectors, columns=feature_names)\n",
    "\n",
    "print(tfidf_data.head()) \n",
    "tfidf_data.to_csv(\"tfidf_output.csv\", index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"Discussions about ChatGPT, its performance, user experiences, applications, limitations, ethical concerns, and comparisons with other AI models developed by OpenAI.\"]\n",
    "\n",
    "idf_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=custom_tokenizer,\n",
    "    use_idf=True,\n",
    "    # turn off sublinear_tf to get the same results as the previous implementation\n",
    "    sublinear_tf=False\n",
    ")\n",
    "\n",
    "idf_vectorizer.fit(data[\"Cleaned Text\"])\n",
    "query_vector = idf_vectorizer.transform([query])\n",
    "\n",
    "similarity_scores = cosine_similarity(query_vector, doc_vectors)[0]\n",
    "\n",
    "data[\"similarity\"] = similarity_scores\n",
    "\n",
    "sorted_data = data.sort_values(by=\"similarity\", ascending=False)\n",
    "\n",
    "sorted_data.to_csv(\"similarity_scores.csv\", index=False)\n",
    "\n",
    "print(\"Data saved to 'similarity_scores.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
