{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install contractions\n",
    "%pip install textblob\n",
    "%pip install nltk\n",
    "%pip install scikit-learn\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "from sklearn.feature_extraction import text\n",
    "import string\n",
    "import contractions\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"combined_data.csv\")\n",
    "data = pd.DataFrame(csv[[\"post_id\", \"comment_id\", \"title\", \"body\"]])\n",
    "data.columns = [\"post_id\", \"comment_id\", \"title\", \"text\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing blank rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=[\"text\"])\n",
    "data = data.reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting texts to lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(text):\n",
    "  return text.lower()\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"text\"].apply(lower)\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing r/, usernames, new line indicators, and links from texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(text):\n",
    "  return re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "def remove_user_mentions(text):\n",
    "    return re.sub(r'u/\\S+', '', text)\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].str.replace('r/', '', regex=False)\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].str.replace(\"\\n\\n\", ' ', regex=False)\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(remove_links)\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(remove_user_mentions)\n",
    "\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing spelling errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(text):\n",
    "    return str(TextBlob(text).correct())\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(correct_spelling)\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding contractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(expand_contractions)\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(\n",
    "    lambda text: ' '.join([word for word in text.split() if word.lower() not in stopwords])\n",
    ")\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing punctuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    text = re.sub(r'[-]', ' ', text)\n",
    "    text = re.sub(r'(\\S)[' + re.escape(string.punctuation) + r'](\\S)', r'\\1 \\2', text)\n",
    "    return text\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(remove_punctuations)\n",
    "\n",
    "pattern_punctuations = r'[' + string.punctuation + r']'\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].str.replace(pattern_punctuations, '', regex=True)\n",
    "\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "def byte_level_tokenizer(text):\n",
    "    byte_sequence = text.encode('utf-8')\n",
    "    latent_tokens = [byte_sequence[i:i+2] for i in range(0, len(byte_sequence), 2)]\n",
    "    return [str(token) for token in latent_tokens]\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=byte_level_tokenizer,\n",
    "    use_idf=False\n",
    ")\n",
    "\n",
    "doc_vectors = vectorizer.fit_transform(data[\"Cleaned Text\"])\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense_vec = doc_vectors.todense()\n",
    "dense_list = dense_vec.tolist()\n",
    "tfidf_data = pd.DataFrame(dense_list, columns=feature_names)\n",
    "tfidf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"Discussions about ChatGPT, its performance, user experiences, applications, limitations, ethical concerns, and comparisons with other AI models developed by OpenAI.\"]\n",
    "\n",
    "idf_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=byte_level_tokenizer,\n",
    "    use_idf=True\n",
    ")\n",
    "\n",
    "idf_vectorizer.fit(data[\"Cleaned Text\"])\n",
    "query_vector = idf_vectorizer.transform([query])\n",
    "\n",
    "similarity_scores = cosine_similarity(query_vector, doc_vectors)[0]\n",
    "\n",
    "data[\"similarity\"] = similarity_scores\n",
    "\n",
    "sorted_data = data.sort_values(by=\"similarity\", ascending=False)\n",
    "\n",
    "sorted_data.to_csv(\"similarity_scores.csv\", index=False)\n",
    "\n",
    "print(\"Data saved to 'similarity_scores.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
