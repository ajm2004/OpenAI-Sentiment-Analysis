{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: emoji in c:\\users\\user\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install contractions\n",
    "# %pip install textblob\n",
    "# %pip install nltk\n",
    "# %pip install scikit-learn\n",
    "# %pip install transformers\n",
    "# %pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44456f5c1c44d3fa216f9094324444c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "from sklearn.feature_extraction import text\n",
    "import string\n",
    "import contractions\n",
    "import emoji\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9184\\1936008020.py:1: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  csv = pd.read_csv(\"combined_data.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1hr4hc6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Weekly Self-Promotional Mega Thread 49, 01.01....</td>\n",
       "      <td>All the self-promotional posts about your AI p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ggixzy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AMA with OpenAI’s Sam Altman, Kevin Weil, Srin...</td>\n",
       "      <td>Consider this AMA our Reddit launch.\\n\\nAsk us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1id5l47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OpenAI Pleads That It Can’t Make Money Without...</td>\n",
       "      <td># \"It would be impossible to train today’s lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1icyjx6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Remember When OpenAI Threatened Your Job? A Fr...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1icvvjq</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"My AI just absolutely roasted me and I'm ques...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id comment_id                                              title  \\\n",
       "0  1hr4hc6        NaN  Weekly Self-Promotional Mega Thread 49, 01.01....   \n",
       "1  1ggixzy        NaN  AMA with OpenAI’s Sam Altman, Kevin Weil, Srin...   \n",
       "2  1id5l47        NaN  OpenAI Pleads That It Can’t Make Money Without...   \n",
       "3  1icyjx6        NaN  Remember When OpenAI Threatened Your Job? A Fr...   \n",
       "4  1icvvjq        NaN  \"My AI just absolutely roasted me and I'm ques...   \n",
       "\n",
       "                                                text  \n",
       "0  All the self-promotional posts about your AI p...  \n",
       "1  Consider this AMA our Reddit launch.\\n\\nAsk us...  \n",
       "2  # \"It would be impossible to train today’s lea...  \n",
       "3                                                NaN  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = pd.read_csv(\"combined_data.csv\")\n",
    "data = pd.DataFrame(csv[[\"post_id\", \"comment_id\", \"title\", \"body\"]])\n",
    "data.columns = [\"post_id\", \"comment_id\", \"title\", \"text\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing blank rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1hr4hc6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Weekly Self-Promotional Mega Thread 49, 01.01....</td>\n",
       "      <td>All the self-promotional posts about your AI p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ggixzy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AMA with OpenAI’s Sam Altman, Kevin Weil, Srin...</td>\n",
       "      <td>Consider this AMA our Reddit launch.\\n\\nAsk us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1id5l47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OpenAI Pleads That It Can’t Make Money Without...</td>\n",
       "      <td># \"It would be impossible to train today’s lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1iclecj</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Already DeepSick of us.</td>\n",
       "      <td>Why are we like this.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1idb44a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I used ChatGPT to beat addiction</td>\n",
       "      <td>Today I'm proud to say that I am 10 days free ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id comment_id                                              title  \\\n",
       "0  1hr4hc6        NaN  Weekly Self-Promotional Mega Thread 49, 01.01....   \n",
       "1  1ggixzy        NaN  AMA with OpenAI’s Sam Altman, Kevin Weil, Srin...   \n",
       "2  1id5l47        NaN  OpenAI Pleads That It Can’t Make Money Without...   \n",
       "3  1iclecj        NaN                            Already DeepSick of us.   \n",
       "4  1idb44a        NaN                   I used ChatGPT to beat addiction   \n",
       "\n",
       "                                                text  \n",
       "0  All the self-promotional posts about your AI p...  \n",
       "1  Consider this AMA our Reddit launch.\\n\\nAsk us...  \n",
       "2  # \"It would be impossible to train today’s lea...  \n",
       "3                             Why are we like this.   \n",
       "4  Today I'm proud to say that I am 10 days free ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Blank Rows\n",
    "data = data.dropna(subset=[\"text\"])\n",
    "data = data.reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting texts to lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    all the self-promotional posts about your ai p...\n",
       "1    consider this ama our reddit launch.\\n\\nask us...\n",
       "2    # \"it would be impossible to train today’s lea...\n",
       "3                               why are we like this. \n",
       "4    today i'm proud to say that i am 10 days free ...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lowercase\n",
    "def lower(text):\n",
    "  return text.lower()\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"text\"].apply(lower)\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing r/, usernames, new line indicators, and links from texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    all the self-promotional posts about your ai p...\n",
       "1    consider this ama our reddit launch. ask us an...\n",
       "2    # \"it would be impossible to train today’s lea...\n",
       "3                               why are we like this. \n",
       "4    today i'm proud to say that i am 10 days free ...\n",
       "Name: Cleaned Text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove unnecessary tokens\n",
    "def remove_links(text):\n",
    "  return re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "def remove_user_mentions(text):\n",
    "    return re.sub(r'u/\\S+', '', text)\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].str.replace('r/', '', regex=False)\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].str.replace(\"\\n\\n\", ' ', regex=False)\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(remove_links)\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(remove_user_mentions)\n",
    "\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing spelling errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix spelling\n",
    "def correct_spelling(text):\n",
    "    return str(TextBlob(text).correct())\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(correct_spelling)\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding contractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand contractions\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(expand_contractions)\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "stopwords = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(\n",
    "    lambda text: ' '.join([word for word in text.split() if word.lower() not in stopwords])\n",
    ")\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing punctuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations\n",
    "def remove_punctuations(text):\n",
    "    text = re.sub(r'[-]', ' ', text)\n",
    "    text = re.sub(r'(\\S)[' + re.escape(string.punctuation) + r'](\\S)', r'\\1 \\2', text)\n",
    "    return text\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(remove_punctuations)\n",
    "\n",
    "pattern_punctuations = r'[' + string.punctuation + r']'\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].str.replace(pattern_punctuations, '', regex=True)\n",
    "\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emojis(text):\n",
    "    \"\"\"\n",
    "    Convert emojis in the text to their descriptive names.\n",
    "    \"\"\"\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "# Apply the conversion to your cleaned text data\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(convert_emojis)\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "def byte_level_tokenizer(text):\n",
    "    byte_sequence = text.encode('utf-8')\n",
    "    latent_tokens = [byte_sequence[i:i+2] for i in range(0, len(byte_sequence), 2)]\n",
    "    return [str(token) for token in latent_tokens]\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=byte_level_tokenizer,\n",
    "    use_idf=False\n",
    ")\n",
    "\n",
    "doc_vectors = vectorizer.fit_transform(data[\"Cleaned Text\"])\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense_vec = doc_vectors.todense()\n",
    "dense_list = dense_vec.tolist()\n",
    "tfidf_data = pd.DataFrame(dense_list, columns=feature_names)\n",
    "tfidf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"Discussions about ChatGPT, its performance, user experiences, applications, limitations, ethical concerns, and comparisons with other AI models developed by OpenAI.\"]\n",
    "\n",
    "idf_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=byte_level_tokenizer,\n",
    "    use_idf=True\n",
    "    # turn off sublinear_tf to get the same results as the previous implementation\n",
    "    sublinear_tf=False\n",
    ")\n",
    "\n",
    "idf_vectorizer.fit(data[\"Cleaned Text\"])\n",
    "query_vector = idf_vectorizer.transform([query])\n",
    "\n",
    "similarity_scores = cosine_similarity(query_vector, doc_vectors)[0]\n",
    "\n",
    "data[\"similarity\"] = similarity_scores\n",
    "\n",
    "sorted_data = data.sort_values(by=\"similarity\", ascending=False)\n",
    "\n",
    "sorted_data.to_csv(\"similarity_scores.csv\", index=False)\n",
    "\n",
    "print(\"Data saved to 'similarity_scores.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
