{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install contractions\n",
    "%pip install textblob\n",
    "%pip install nltk\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_extraction import text\n",
    "import string\n",
    "import contractions\n",
    "from textblob import TextBlob\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"combined_data.csv\")\n",
    "data = pd.DataFrame(csv[[\"post_id\", \"comment_id\", \"title\", \"body\"]])\n",
    "data.columns = [\"post_id\", \"comment_id\", \"title\", \"text\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing blank rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=[\"text\"])\n",
    "data = data.reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting texts to lowercase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(text):\n",
    "  return text.lower()\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"text\"].apply(lower)\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing r/, usernames, new line indicators, and links from texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(text):\n",
    "  return re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "def remove_user_mentions(text):\n",
    "    return re.sub(r'u/\\S+', '', text)\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].str.replace('r/', '', regex=False)\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].str.replace(\"\\n\\n\", ' ', regex=False)\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(remove_links)\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(remove_user_mentions)\n",
    "\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing spelling errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(text):\n",
    "    return str(TextBlob(text).correct())\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(correct_spelling)\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding contractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(expand_contractions)\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].apply(\n",
    "    lambda text: ' '.join([word for word in text.split() if word.lower() not in stopwords])\n",
    ")\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing punctuations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_punctuations = r'[' + string.punctuation + r']'\n",
    "\n",
    "data[\"Cleaned Text\"] = data[\"Cleaned Text\"].str.replace(pattern_punctuations, '', regex=True)\n",
    "data[\"Cleaned Text\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using KMeans clustering to cluster text into 3 clusters and calculating the best silhouette score in order to find the best paramets for Tf-idf vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = -1\n",
    "best_params = None\n",
    "\n",
    "param_grid = [\n",
    "    {'max_df': 1.0, 'min_df': 1, 'ngram_range': (1, 1), 'use_idf': True}, \n",
    "    {'max_df': 0.8, 'min_df': 2, 'ngram_range': (1, 1), 'use_idf': True}, \n",
    "    {'max_df': 1.0, 'min_df': 1, 'ngram_range': (1, 2), 'use_idf': True}, \n",
    "    {'max_df': 1.0, 'min_df': 1, 'ngram_range': (1, 1), 'use_idf': False},\n",
    "    {'max_df': 0.7, 'min_df': 2, 'ngram_range': (1, 2), 'use_idf': True},\n",
    "    {'max_df': 1.0, 'min_df': 1, 'ngram_range': (1, 3), 'use_idf': True},\n",
    "    {'max_df': 1.0, 'min_df': 1, 'ngram_range': (1, 3), 'use_idf': False},\n",
    "    {'max_df': 0.5, 'min_df': 3, 'ngram_range': (1, 1), 'use_idf': True},\n",
    "    {'max_df': 1.0, 'min_df': 1, 'ngram_range': (2, 3), 'use_idf': False},\n",
    "    {'max_df': 0.9, 'min_df': 1, 'ngram_range': (1, 1), 'use_idf': True},\n",
    "    {'max_df': 1.0, 'min_df': 1, 'ngram_range': (1, 3), 'use_idf': True},\n",
    "    {'max_df': 0.6, 'min_df': 4, 'ngram_range': (1, 3), 'use_idf': True},\n",
    "    {'max_df': 1.0, 'min_df': 2, 'ngram_range': (2, 2), 'use_idf': False},\n",
    "    {'max_df': 0.4, 'min_df': 5, 'ngram_range': (1, 1), 'use_idf': True},\n",
    "    {'max_df': 0.75, 'min_df': 3, 'ngram_range': (1, 2), 'use_idf': True}\n",
    "]\n",
    "\n",
    "for params in param_grid:\n",
    "    vectorizer = TfidfVectorizer(**params)\n",
    "    tfidf_matrix = vectorizer.fit_transform(data[\"Cleaned Text\"])\n",
    "    \n",
    "    num_clusters = 3\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(tfidf_matrix)\n",
    "\n",
    "    score = silhouette_score(tfidf_matrix, cluster_labels)\n",
    "    \n",
    "    print(f\"Params: {params}, Silhouette Score: {score:.4f}\")\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = params\n",
    "\n",
    "print(f\"\\nBest TF-IDF Parameters: {best_params} with Score: {best_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(**best_params)\n",
    "vectors = vectorizer.fit_transform(data[\"Cleaned Text\"])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense_vec = vectors.todense()\n",
    "dense_list = dense_vec.tolist()\n",
    "tfidf_data = pd.DataFrame(dense_list, columns=feature_names)\n",
    "tfidf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"Discussions about ChatGPT, its performance, user experiences, applications, limitations, ethical concerns, and comparisons with other AI models developed by OpenAI.\"]\n",
    "\n",
    "query_tfidf = vectorizer.transform(query)\n",
    "query_sim = cosine_similarity(query_tfidf, tfidf_matrix)[0]\n",
    "\n",
    "data[\"similarity\"] = query_sim\n",
    "\n",
    "sorted_data = data.sort_values(by=\"similarity\", ascending=False)\n",
    "\n",
    "sorted_data.to_csv(\"similarity_scores.csv\", index=False)\n",
    "\n",
    "print(\"Data saved to 'similarity_scores.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
