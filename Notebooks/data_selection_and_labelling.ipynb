{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA SELECTION AND LABELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LOADING AND BASIC PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the raw data extracted from Reddit\n",
    "raw_data = pd.read_csv('../Data/new_combined_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   post_id   subreddit                                        post_title  \\\n",
      "0  1002dom  technology  ChatGPT Caused 'Code Red' at Google, Report Says   \n",
      "1  1002dom  technology  ChatGPT Caused 'Code Red' at Google, Report Says   \n",
      "2  1002dom  technology  ChatGPT Caused 'Code Red' at Google, Report Says   \n",
      "3  1002dom  technology  ChatGPT Caused 'Code Red' at Google, Report Says   \n",
      "4  1002dom  technology  ChatGPT Caused 'Code Red' at Google, Report Says   \n",
      "\n",
      "  post_body  number_of_comments   readable_datetime post_author comment_id  \\\n",
      "0       nan                 370 2023-01-01 00:03:33    slakmehl    j2far1e   \n",
      "1       nan                 370 2023-01-01 00:03:33    slakmehl    j2f5vg2   \n",
      "2       nan                 370 2023-01-01 00:03:33    slakmehl    j2f9y5m   \n",
      "3       nan                 370 2023-01-01 00:03:33    slakmehl    j2f7njc   \n",
      "4       nan                 370 2023-01-01 00:03:33    slakmehl    j2fna2c   \n",
      "\n",
      "                                        comment_body  number_of_upvotes  \\\n",
      "0                    Chat GPT wrote this article ffs                792   \n",
      "1                        Did you order the code red?                687   \n",
      "2  If my search engine was littered with SEO keyw...               1288   \n",
      "3    How many more times are we gonna see this story                306   \n",
      "4  How far can we trust ChatGPT? It's very intere...                 70   \n",
      "\n",
      "        comment_author    query  \n",
      "0  The_Bridge_Imperium  ChatGPT  \n",
      "1            damienn22  ChatGPT  \n",
      "2              1x2x4x1  ChatGPT  \n",
      "3            frombaktk  ChatGPT  \n",
      "4         Milk_Busters  ChatGPT  \n"
     ]
    }
   ],
   "source": [
    "# Declare each field's data type\n",
    "raw_data['post_id'] = raw_data['post_id'].astype(str)\n",
    "raw_data['comment_id'] = raw_data['comment_id'].astype(str)\n",
    "raw_data['post_title'] = raw_data['post_title'].astype(str)\n",
    "raw_data['post_body'] = raw_data['post_body'].astype(str)\n",
    "raw_data['post_author'] = raw_data['post_author'].astype(str)\n",
    "raw_data['comment_body'] = raw_data['comment_body'].astype(str)\n",
    "raw_data['comment_author'] = raw_data['comment_author'].astype(str)\n",
    "raw_data['query'] = raw_data['query'].astype(str)\n",
    "\n",
    "raw_data['subreddit'] = raw_data['subreddit'].astype('category')\n",
    "raw_data['query'] = raw_data['query'].astype('category')\n",
    "\n",
    "# Fill NaN values with 0 before converting to int\n",
    "raw_data['number_of_comments'] = raw_data['number_of_comments'].fillna(0).astype(int)\n",
    "raw_data['number_of_upvotes'] = raw_data['number_of_upvotes'].fillna(0).astype(int)\n",
    "\n",
    "raw_data['readable_datetime'] = pd.to_datetime(raw_data['readable_datetime'])\n",
    "\n",
    "print(raw_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HEURISTIC FILTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty texts: 0\n",
      "Number of NA texts: 0\n",
      "Total records: 54966\n",
      "\n",
      "Number of posts: 9575\n",
      "\n",
      " After adding new row for all unique posts\n",
      "\n",
      "Number of empty texts: 0\n",
      "Number of NA texts: 0\n",
      "Total records: 64541\n"
     ]
    }
   ],
   "source": [
    "# Create combined text field and replace None/NaN with empty string\n",
    "raw_data[\"text\"] = raw_data.apply(\n",
    "\tlambda row: (\n",
    "\t\tstr(row['comment_body']).strip() if pd.notna(row['comment_body'])\n",
    "\t\telse \"\"), \n",
    "\taxis=1\n",
    ").fillna(\"\")\n",
    "\n",
    "# Print count\n",
    "print(\"Number of empty texts:\", (raw_data[\"text\"] == \"\").sum())\n",
    "print(\"Number of NA texts:\", raw_data[\"text\"].isna().sum())\n",
    "print(\"Total records:\", len(raw_data))\n",
    "\n",
    "# Drop rows where text is empty\n",
    "raw_data = raw_data[raw_data[\"text\"] != \"\"]\n",
    "\n",
    "# Add a new row for all unique posts (get post_id, comments, and all from first record of the post, exclude any comment fields)\n",
    "posts = raw_data.groupby(\"post_id\").first().reset_index()\n",
    "posts = posts.drop(columns=[\"comment_id\", \"comment_body\", \"comment_author\",\"text\"])\n",
    "\n",
    "print(\"\\nNumber of posts:\", len(posts))\n",
    "\n",
    "# Make post text the as post title and post body\n",
    "posts[\"text\"] = posts[\"post_title\"] + \" \" + posts[\"post_body\"]\n",
    "\n",
    "# concat raw data and posts\n",
    "raw_data = pd.concat([posts, raw_data], ignore_index=True)\n",
    "\n",
    "# Remove any duplicate rows\n",
    "raw_data = raw_data.drop_duplicates()\n",
    "\n",
    "# Display first few rows and value counts of empty strings\n",
    "print(\"\\n After adding new row for all unique posts\")\n",
    "print(\"\\nNumber of empty texts:\", (raw_data[\"text\"] == \"\").sum())\n",
    "print(\"Number of NA texts:\", raw_data[\"text\"].isna().sum())\n",
    "print(\"Total records:\", len(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Posts and Comments:\n",
      "9575\n",
      "54966\n"
     ]
    }
   ],
   "source": [
    "# No.of Posts and Comments\n",
    "print(\"\\nNumber of Posts and Comments:\")\n",
    "print(raw_data[\"post_id\"].nunique())\n",
    "print(raw_data[\"comment_id\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = raw_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  S1: Filter to past 5 year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Posts and Comments after filtering:\n",
      "9544\n",
      "54905\n",
      "\n",
      "Date time range:\n",
      "2020-05-01 13:26:05\n",
      "2025-01-30 16:11:12\n"
     ]
    }
   ],
   "source": [
    "cutoff_date = datetime.now() - timedelta(days=5*365)\n",
    "\n",
    "filtered_data = filtered_data[filtered_data[\"readable_datetime\"] > cutoff_date]\n",
    "\n",
    "print(\"\\nNumber of Posts and Comments after filtering:\")\n",
    "print(filtered_data[\"post_id\"].nunique())\n",
    "print(filtered_data[\"comment_id\"].count())\n",
    "\n",
    "# Print date time range in the data\n",
    "print(\"\\nDate time range:\")\n",
    "print(filtered_data[\"readable_datetime\"].min())\n",
    "print(filtered_data[\"readable_datetime\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all missing records where text is empty\n",
    "filtered_data = filtered_data[filtered_data[\"text\"] != \"\"]\n",
    "filtered_data = filtered_data[filtered_data[\"text\"].notna()]\n",
    "filtered_data = filtered_data[filtered_data[\"text\"] != \"nan\"]\n",
    "filtered_data = filtered_data[filtered_data[\"text\"] != \"None\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2: Text Length\n",
    "\n",
    "Short Texts: Extremely short texts (e.g., those with only one or two words) might not provide enough context and could be noise.\n",
    "\n",
    "Excessively Long Texts: Conversely, texts that far exceed the typical length for your domain might be off-topic or contain noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word count statistics:\n",
      "  count     mean      std    min    25%    50%    75%    max\n",
      "-------  -------  -------  -----  -----  -----  -----  -----\n",
      "  63852  53.8432  139.942      1     10     22     55   5827\n",
      "\n",
      "\n",
      "Max words set to: 122\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "min_words = 3\n",
    "\n",
    "# Calculate word counts for each text\n",
    "word_counts = filtered_data['text'].str.split().str.len()\n",
    "\n",
    "print(\"\\nWord count statistics:\")\n",
    "print(tabulate([word_counts.describe()], headers='keys'))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Set max_words as the upper quartile (75th percentile) plus 1.5 times IQR\n",
    "Q3 = word_counts.quantile(0.75)\n",
    "Q1 = word_counts.quantile(0.25)\n",
    "IQR = Q3 - Q1\n",
    "max_words = int(Q3 + 1.5 * IQR)\n",
    "\n",
    "print(f\"Max words set to: {max_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9544\n",
      "9544\n",
      "\n",
      "Number of Posts and Comments after filtering by word count:\n",
      "9332\n",
      "7066\n"
     ]
    }
   ],
   "source": [
    "print(filtered_data[\"post_id\"].nunique())\n",
    "print(filtered_data[\"comment_id\"].isna().sum())\n",
    "\n",
    "# First recalculate word counts since filtered_data has been modified since last count\n",
    "word_counts = filtered_data['text'].str.split().str.len()\n",
    "\n",
    "# Filter based on min and max words\n",
    "filtered_data = filtered_data[word_counts.between(min_words, max_words)]\n",
    "\n",
    "print(\"\\nNumber of Posts and Comments after filtering by word count:\")\n",
    "print(filtered_data[\"post_id\"].nunique())\n",
    "print(filtered_data[\"comment_id\"].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SEMANTIC SEARCH WITH SENTENCE TRANSFORMERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
