{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA SELECTION AND LABELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tabulate sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LOADING AND BASIC PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the raw data extracted from Reddit\n",
    "raw_data = pd.read_csv('../Data/new_combined_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   post_id   subreddit                                        post_title  \\\n",
      "0  1002dom  technology  ChatGPT Caused 'Code Red' at Google, Report Says   \n",
      "1  1002dom  technology  ChatGPT Caused 'Code Red' at Google, Report Says   \n",
      "2  1002dom  technology  ChatGPT Caused 'Code Red' at Google, Report Says   \n",
      "3  1002dom  technology  ChatGPT Caused 'Code Red' at Google, Report Says   \n",
      "4  1002dom  technology  ChatGPT Caused 'Code Red' at Google, Report Says   \n",
      "\n",
      "  post_body  number_of_comments   readable_datetime post_author comment_id  \\\n",
      "0       nan                 370 2023-01-01 00:03:33    slakmehl    j2far1e   \n",
      "1       nan                 370 2023-01-01 00:03:33    slakmehl    j2f5vg2   \n",
      "2       nan                 370 2023-01-01 00:03:33    slakmehl    j2f9y5m   \n",
      "3       nan                 370 2023-01-01 00:03:33    slakmehl    j2f7njc   \n",
      "4       nan                 370 2023-01-01 00:03:33    slakmehl    j2fna2c   \n",
      "\n",
      "                                        comment_body  number_of_upvotes  \\\n",
      "0                    Chat GPT wrote this article ffs                792   \n",
      "1                        Did you order the code red?                687   \n",
      "2  If my search engine was littered with SEO keyw...               1288   \n",
      "3    How many more times are we gonna see this story                306   \n",
      "4  How far can we trust ChatGPT? It's very intere...                 70   \n",
      "\n",
      "        comment_author    query  \n",
      "0  The_Bridge_Imperium  ChatGPT  \n",
      "1            damienn22  ChatGPT  \n",
      "2              1x2x4x1  ChatGPT  \n",
      "3            frombaktk  ChatGPT  \n",
      "4         Milk_Busters  ChatGPT  \n"
     ]
    }
   ],
   "source": [
    "# Declare each field's data type\n",
    "raw_data['post_id'] = raw_data['post_id'].astype(str)\n",
    "raw_data['comment_id'] = raw_data['comment_id'].astype(str)\n",
    "raw_data['post_title'] = raw_data['post_title'].astype(str)\n",
    "raw_data['post_body'] = raw_data['post_body'].astype(str)\n",
    "raw_data['post_author'] = raw_data['post_author'].astype(str)\n",
    "raw_data['comment_body'] = raw_data['comment_body'].astype(str)\n",
    "raw_data['comment_author'] = raw_data['comment_author'].astype(str)\n",
    "raw_data['query'] = raw_data['query'].astype(str)\n",
    "\n",
    "raw_data['subreddit'] = raw_data['subreddit'].astype('category')\n",
    "raw_data['query'] = raw_data['query'].astype('category')\n",
    "\n",
    "# Fill NaN values with 0 before converting to int\n",
    "raw_data['number_of_comments'] = raw_data['number_of_comments'].fillna(0).astype(int)\n",
    "raw_data['number_of_upvotes'] = raw_data['number_of_upvotes'].fillna(0).astype(int)\n",
    "\n",
    "raw_data['readable_datetime'] = pd.to_datetime(raw_data['readable_datetime'])\n",
    "\n",
    "print(raw_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HEURISTIC FILTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty texts: 0\n",
      "Number of NA texts: 0\n",
      "Total records: 54966\n",
      "\n",
      "Number of posts: 9575\n",
      "\n",
      " After adding new row for all unique posts\n",
      "\n",
      "Number of empty texts: 0\n",
      "Number of NA texts: 0\n",
      "Total records: 64541\n"
     ]
    }
   ],
   "source": [
    "# Create combined text field and replace None/NaN with empty string\n",
    "raw_data[\"text\"] = raw_data.apply(\n",
    "\tlambda row: (\n",
    "\t\tstr(row['comment_body']).strip() if pd.notna(row['comment_body'])\n",
    "\t\telse \"\"), \n",
    "\taxis=1\n",
    ").fillna(\"\")\n",
    "\n",
    "# Print count\n",
    "print(\"Number of empty texts:\", (raw_data[\"text\"] == \"\").sum())\n",
    "print(\"Number of NA texts:\", raw_data[\"text\"].isna().sum())\n",
    "print(\"Total records:\", len(raw_data))\n",
    "\n",
    "# Drop rows where text is empty\n",
    "raw_data = raw_data[raw_data[\"text\"] != \"\"]\n",
    "\n",
    "# Add a new row for all unique posts (get post_id, comments, and all from first record of the post, exclude any comment fields)\n",
    "posts = raw_data.groupby(\"post_id\").first().reset_index()\n",
    "posts = posts.drop(columns=[\"comment_id\", \"comment_body\", \"comment_author\",\"text\"])\n",
    "\n",
    "print(\"\\nNumber of posts:\", len(posts))\n",
    "\n",
    "# Make post text the as post title and post body\n",
    "posts[\"text\"] = posts[\"post_title\"] + \" \" + posts[\"post_body\"]\n",
    "\n",
    "# concat raw data and posts\n",
    "raw_data = pd.concat([posts, raw_data], ignore_index=True)\n",
    "\n",
    "# Remove any duplicate rows\n",
    "raw_data = raw_data.drop_duplicates()\n",
    "\n",
    "# Display first few rows and value counts of empty strings\n",
    "print(\"\\n After adding new row for all unique posts\")\n",
    "print(\"\\nNumber of empty texts:\", (raw_data[\"text\"] == \"\").sum())\n",
    "print(\"Number of NA texts:\", raw_data[\"text\"].isna().sum())\n",
    "print(\"Total records:\", len(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Posts and Comments:\n",
      "9575\n",
      "54966\n"
     ]
    }
   ],
   "source": [
    "# No.of Posts and Comments\n",
    "print(\"\\nNumber of Posts and Comments:\")\n",
    "print(raw_data[\"post_id\"].nunique())\n",
    "print(raw_data[\"comment_id\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = raw_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  S1: Filter to past 5 year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Posts and Comments after filtering:\n",
      "9544\n",
      "54905\n",
      "\n",
      "Date time range:\n",
      "2020-05-01 13:26:05\n",
      "2025-01-30 16:11:12\n"
     ]
    }
   ],
   "source": [
    "cutoff_date = datetime.now() - timedelta(days=5*365)\n",
    "\n",
    "filtered_data = filtered_data[filtered_data[\"readable_datetime\"] > cutoff_date]\n",
    "\n",
    "print(\"\\nNumber of Posts and Comments after filtering:\")\n",
    "print(filtered_data[\"post_id\"].nunique())\n",
    "print(filtered_data[\"comment_id\"].count())\n",
    "\n",
    "# Print date time range in the data\n",
    "print(\"\\nDate time range:\")\n",
    "print(filtered_data[\"readable_datetime\"].min())\n",
    "print(filtered_data[\"readable_datetime\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all missing records where text is empty\n",
    "filtered_data = filtered_data[filtered_data[\"text\"] != \"\"]\n",
    "filtered_data = filtered_data[filtered_data[\"text\"].notna()]\n",
    "filtered_data = filtered_data[filtered_data[\"text\"] != \"nan\"]\n",
    "filtered_data = filtered_data[filtered_data[\"text\"] != \"None\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2: Text Length\n",
    "\n",
    "Short Texts: Extremely short texts (e.g., those with only one or two words) might not provide enough context and could be noise.\n",
    "\n",
    "Excessively Long Texts: Conversely, texts that far exceed the typical length for your domain might be off-topic or contain noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word count statistics:\n",
      "  count     mean      std    min    25%    50%    75%    max\n",
      "-------  -------  -------  -----  -----  -----  -----  -----\n",
      "  63852  53.8432  139.942      1     10     22     55   5827\n",
      "\n",
      "\n",
      "Max words set to: 122\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "min_words = 3\n",
    "\n",
    "# Calculate word counts for each text\n",
    "word_counts = filtered_data['text'].str.split().str.len()\n",
    "\n",
    "print(\"\\nWord count statistics:\")\n",
    "print(tabulate([word_counts.describe()], headers='keys'))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Set max_words as the upper quartile (75th percentile) plus 1.5 times IQR\n",
    "Q3 = word_counts.quantile(0.75)\n",
    "Q1 = word_counts.quantile(0.25)\n",
    "IQR = Q3 - Q1\n",
    "max_words = int(Q3 + 1.5 * IQR)\n",
    "\n",
    "print(f\"Max words set to: {max_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9332\n",
      "7066\n",
      "\n",
      "Number of Posts and Comments after filtering by word count:\n",
      "9332\n",
      "47505\n"
     ]
    }
   ],
   "source": [
    "print(filtered_data[\"post_id\"].nunique())\n",
    "print(filtered_data[\"comment_id\"].isna().sum())\n",
    "\n",
    "# First recalculate word counts since filtered_data has been modified since last count\n",
    "word_counts = filtered_data['text'].str.split().str.len()\n",
    "\n",
    "# Filter based on min and max words\n",
    "filtered_data = filtered_data[word_counts.between(min_words, max_words)]\n",
    "\n",
    "print(\"\\nNumber of Posts and Comments after filtering by word count:\")\n",
    "print(filtered_data[\"post_id\"].nunique())\n",
    "print(filtered_data[\"comment_id\"].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SEMANTIC SEARCH WITH SENTENCE TRANSFORMERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filtering, there ~54k records (~9k posts, ~47k comments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this section is to select the most relevant records that express sentiments about OpenAI, and filter out low quality data. It will enable us to produce a high quality dataset for company reputation analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to using embedding-based semantic search, we experimented with TF-IDF-based retrieval, to find the most relevant records, i.e, the records with the highest cosine similarity to a given query. However, upon manually labellign ~450 of the most relevant records selected using TF-IDF, we found that ~41% of the records were irrelevant, i.e, they express no positive/negative/neutral sentiment about OpenAI.\n",
    "\n",
    "This is primarily because term-based vectorization methods like TF-IDF do not represent the semantic meaning of the data. Therefore, we decided to experiment with using embedding models with the Sentence Transformers library, which are specialized for conducting semantic retrieval of the most relevant data points, using cosine similarity.\n",
    "\n",
    "You can find our experiments with retrieval using TF-IDF here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are utilizing the msmarco-distilbert-cos-v5 model as the embedding model for the following reasons:\n",
    "1. As visualized during exploratory data analysis, our \"passages\" (comments and posts) are generally longer than the length of the queries we will be using for retrieval (see below). Therefore, we require a model for asymmetric semantic search (where the query is generally shorter in length than the passages to be retrieved). The [Sentence Transformer documentation](https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search) recommends models trained on the MS-MARCO information retrieval dataset, for asymmetric semantic search. \n",
    "\n",
    "2. DistilBERT is a smaller, lighter version of BERT that maintains most of the original performance. It is used as the backbone of this embedding model. Therefore, it will be efficient and quick to retrieve relevant examples from our dataset. \n",
    "\n",
    "3. The model performs relatively well compared to other Sentence Transformers on various [information retrieval benchmarks](https://www.sbert.net/docs/pretrained-models/msmarco-v5.html#performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jonathan\\Desktop\\F20AA - Applied Text Analytics\\CW1\\F20AA_Grp5\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Jonathan\\.cache\\huggingface\\hub\\models--sentence-transformers--msmarco-distilbert-cos-v5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# Load the embedding model\n",
    "embedding_model = SentenceTransformer(\"msmarco-distilbert-cos-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple search queries, corresponding to each sentiment label, to help\n",
    "# retrieve a balanced dataset\n",
    "queries = [\"Positive reviews about OpenAI and its products.\",\n",
    "           \"Negative reviews about OpenAI and its products.\",\n",
    "           \"Neutral reviews about OpenAI and its products.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the text column of filtered_data as a list \n",
    "reviews = filtered_data[\"text\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for the queries\n",
    "query_embeddings = embedding_model.encode(queries, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for the reviews\n",
    "review_embeddings = embedding_model.encode(reviews[:15], convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cosine similarity search between the queries and reviews embeddings, and retrieve the top 5000 most similar reviews, for each query\n",
    "retrieved_reviews = util.semantic_search(query_embeddings, review_embeddings, top_k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'corpus_id': 6, 'score': 0.44747480750083923},\n",
       "  {'corpus_id': 10, 'score': 0.21365728974342346},\n",
       "  {'corpus_id': 7, 'score': 0.2047780156135559},\n",
       "  {'corpus_id': 9, 'score': 0.14527137577533722},\n",
       "  {'corpus_id': 14, 'score': 0.13351096212863922}],\n",
       " [{'corpus_id': 6, 'score': 0.4552645683288574},\n",
       "  {'corpus_id': 7, 'score': 0.23623406887054443},\n",
       "  {'corpus_id': 10, 'score': 0.218885600566864},\n",
       "  {'corpus_id': 14, 'score': 0.18830770254135132},\n",
       "  {'corpus_id': 9, 'score': 0.11210364103317261}],\n",
       " [{'corpus_id': 6, 'score': 0.5170031189918518},\n",
       "  {'corpus_id': 10, 'score': 0.257534921169281},\n",
       "  {'corpus_id': 7, 'score': 0.2217705100774765},\n",
       "  {'corpus_id': 13, 'score': 0.17739950120449066},\n",
       "  {'corpus_id': 2, 'score': 0.1281353384256363}]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
