{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics Pipeline for Text Classification\n",
    "\n",
    "This notebook demonstrates how to build a text analytics pipeline that includes text processing, feature extraction, classification, and evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install below dependancies/modules in order to to re-run this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy nltk emoji spacy contractions scikit-learn imbalanced-learn\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Download the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline to load or download the dataset\n",
    "\"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Download GloVe pretrained word vectors :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline to load or download the dataset\n",
    "\"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer  \n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "import spacy\n",
    "import contractions\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Text Preprocessor\n",
    "\n",
    "The custom transformer below:\n",
    "\n",
    " - **Emoji Conversion:** Converts any emojis to their text descriptions.\n",
    " - **Normalization:** Lowercases the text.\n",
    " - **Punctuation Removal:** Removes punctuation using regex.\n",
    " - **Tokenization:** Uses NLTKâ€™s `word_tokenize`.\n",
    " - **Stop-word Removal:** Filters out English stopwords.\n",
    " - **Stemming:** Applies Porter stemming.\n",
    " \n",
    " The transformer implements `fit` and `transform` so that it can be used inside a scikit-learn pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, do_stemming=True, do_lemmatization=False, remove_stopwords=True, \n",
    "                 do_emoji_conversion=True, use_spacy_tokenizer=True):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - do_stemming: Apply stemming (reduces words to their root form)\n",
    "        - do_lemmatization: Apply lemmatization (converts words to their canonical form)\n",
    "          Note: When using the default (NLTK) tokenizer, if both do_lemmatization and do_stemming are enabled,\n",
    "          lemmatization takes precedence.\n",
    "        - remove_stopwords: Remove common stopwords\n",
    "        - do_emoji_conversion: Convert emojis to text descriptions\n",
    "        - use_spacy_tokenizer: Use a custom spaCy-based tokenizer (which already uses lemmatization)\n",
    "        \"\"\"\n",
    "        self.do_stemming = do_stemming\n",
    "        self.do_lemmatization = do_lemmatization\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.do_emoji_conversion = do_emoji_conversion\n",
    "        self.use_spacy_tokenizer = use_spacy_tokenizer\n",
    "        self.stemmer = PorterStemmer()\n",
    "        if self.do_lemmatization:\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Load the spaCy model if using the spaCy tokenizer\n",
    "        if self.use_spacy_tokenizer:\n",
    "            self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    def remove_links(self, text):\n",
    "        \"\"\"Remove URLs from text.\"\"\"\n",
    "        return re.sub(r'http[s]?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    def remove_user_mentions(self, text):\n",
    "        \"\"\"Remove user mentions from text.\"\"\"\n",
    "        return re.sub(r'u/\\S+', '', text)\n",
    "    \n",
    "    def expand_contractions(self, text):\n",
    "        \"\"\"Expand contractions in the text.\"\"\"\n",
    "        return contractions.fix(text)\n",
    "    \n",
    "    def remove_non_ascii(self, text):\n",
    "        \"\"\"Remove non-ASCII characters from the text.\"\"\"\n",
    "        return text.encode(\"ascii\", \"ignore\").decode()\n",
    "    \n",
    "    def remove_punctuations(self, text):\n",
    "        \"\"\"\n",
    "        Remove or adjust punctuation in text.\n",
    "        Replaces hyphens with space and ensures separation around punctuation.\n",
    "        \"\"\"\n",
    "        text = re.sub(r'[-]', ' ', text)\n",
    "        text = re.sub(r'(\\S)[' + re.escape(string.punctuation) + r'](\\S)', r'\\1 \\2', text)\n",
    "        return text\n",
    "    \n",
    "    def remove_numbers(self, text):\n",
    "        \"\"\"Remove numbers from text.\"\"\"\n",
    "        return re.sub(r'[0-9]+', '', text)\n",
    "    \n",
    "    def emoji_to_text(self, text):\n",
    "        \"\"\"Convert emojis to text descriptions.\"\"\"\n",
    "        return emoji.demojize(text)\n",
    "    \n",
    "    def normalize(self, text):\n",
    "        \"\"\"Lowercase the text.\"\"\"\n",
    "        return text.lower()\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize text using either a spaCy-based custom tokenizer or the default NLTK tokenizer.\n",
    "        \"\"\"\n",
    "        if self.use_spacy_tokenizer:\n",
    "            # Use spaCy's custom tokenization logic:\n",
    "            doc = self.nlp(text)\n",
    "            tokens = []\n",
    "            # Add named entities as tokens\n",
    "            for ent in doc.ents:\n",
    "                tokens.append(ent.text)\n",
    "            # Add non-entity tokens using their lemma\n",
    "            non_entity_tokens = [token.lemma_.lower() for token in doc if not token.ent_type_ \n",
    "                                 and not token.is_punct and not token.is_space]\n",
    "            tokens.extend(non_entity_tokens)\n",
    "            if self.remove_stopwords:\n",
    "                tokens = [token for token in tokens if token.lower() not in self.stop_words]\n",
    "            if self.do_stemming:\n",
    "                tokens = [self.stemmer.stem(token) for token in tokens]\n",
    "            return tokens\n",
    "        else:\n",
    "            # Default NLTK-based tokenization:\n",
    "            # Remove punctuation (if any remains) and then tokenize\n",
    "            text = re.sub(r'[^\\w\\s]', '', text)\n",
    "            tokens = word_tokenize(text)\n",
    "            # Keep only alphabetic tokens\n",
    "            tokens = [token for token in tokens if token.isalpha()]\n",
    "            if self.remove_stopwords:\n",
    "                tokens = [token for token in tokens if token.lower() not in self.stop_words]\n",
    "            # Apply lemmatization if enabled; otherwise, apply stemming if enabled\n",
    "            if self.do_lemmatization:\n",
    "                tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "            elif self.do_stemming:\n",
    "                tokens = [self.stemmer.stem(token) for token in tokens]\n",
    "            return tokens\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Apply the complete preprocessing pipeline to the text.\"\"\"\n",
    "        text = self.remove_links(text)\n",
    "        text = self.remove_user_mentions(text)\n",
    "        text = self.expand_contractions(text)\n",
    "        text = self.remove_non_ascii(text)\n",
    "        text = self.remove_punctuations(text)\n",
    "        text = self.remove_numbers(text)\n",
    "        if self.do_emoji_conversion:\n",
    "            text = self.emoji_to_text(text)\n",
    "        text = self.normalize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X.apply(self.preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloveVectorizer Class\n",
    "\n",
    "In our project, the `GloveVectorizer` is used to transform text data into numerical features by leveraging pre-trained GloVe embeddings. This approach provides semantic-rich, dense vector representations of documents, which can improve model performance over traditional sparse representations.\n",
    "\n",
    " **GloVe Word Vector Used:**  glove.twitter.27B.50d.txt \n",
    "\n",
    "### Key Components\n",
    "\n",
    "- **`__init__`:**  \n",
    "  Initializes the vectorizer with the GloVe file path and embedding dimension.\n",
    "\n",
    "- **`fit`:**  \n",
    "  Loads the GloVe embeddings into a dictionary for quick lookup.\n",
    "\n",
    "- **`transform`:**  \n",
    "  Converts each document into an average embedding vector by:\n",
    "  - Splitting the text into tokens.\n",
    "  - Retrieving the corresponding embedding for each token.\n",
    "  - Averaging these embeddings to form a single vector for the document.\n",
    "\n",
    "This vectorizer is essential for capturing the contextual meaning of words, enhancing the classifier's ability to understand and process text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloveVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Loading pre-trained GloVe embeddings and returns the average embedding vector for each document.\n",
    "    \"\"\"\n",
    "    def __init__(self, glove_file='glove.twitter.27B.50d.txt', embedding_dim=50):\n",
    "        self.glove_file = glove_file\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.embeddings_index = {}\n",
    "        with open(self.glove_file, encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                self.embeddings_index[word] = coefs\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        vectors = []\n",
    "        for doc in X:\n",
    "            # Since TextPreprocessor returns a space-separated string of tokens,\n",
    "            # we can simply split on spaces.\n",
    "            tokens = doc.split()\n",
    "            token_vecs = [self.embeddings_index[token] for token in tokens if token in self.embeddings_index]\n",
    "            if token_vecs:\n",
    "                doc_vec = np.mean(token_vecs, axis=0)\n",
    "            else:\n",
    "                doc_vec = np.zeros(self.embedding_dim)\n",
    "            vectors.append(doc_vec)\n",
    "        return np.array(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Data Loading and Train/Test Split\n",
    " \n",
    " We load the dataset and split it into training (80%) and testing (20%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in dataset: ['post_id', 'subreddit', 'post_title', 'post_body', 'number_of_comments', 'readable_datetime', 'post_author', 'number_of_upvotes', 'query', 'text', 'comment_id', 'comment_body', 'comment_author', 'label']\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset \n",
    "df = pd.read_csv(\"../Data/labelled_data.csv\")\n",
    "\n",
    "# Check available columns\n",
    "print(\"Columns in dataset:\", df.columns.tolist())\n",
    "\n",
    "# Select the important columns and drop any missing values\n",
    "df = df[['text', 'label']].dropna()\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Basic Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipelines for Sentiment Analysis\n",
    "\n",
    "This section defines a variety of pipelines to preprocess text data and train classifiers using different vectorization methods. Each pipeline uses a custom text preprocessor and is configured to handle class imbalance via weighted class balancing (or random under-sampling for Naive Bayes).\n",
    "\n",
    "## Pipeline Categories\n",
    "\n",
    "- **Logistic Regression Pipelines**  \n",
    "  Pipelines using both CountVectorizer (with unigrams and n-grams) and TfidfVectorizer (with unigrams and n-grams) paired with Logistic Regression.\n",
    "\n",
    "- **SVM Pipelines**  \n",
    "  Pipelines combining CountVectorizer or TfidfVectorizer (with unigrams and n-grams) with LinearSVC, with class weights balanced.\n",
    "\n",
    "- **Random Forest Pipelines**  \n",
    "  Similar to the above, these pipelines use CountVectorizer or TfidfVectorizer (with unigrams and n-grams) with a RandomForestClassifier configured for balanced classes.\n",
    "\n",
    "- **Naive Bayes Pipelines**  \n",
    "  For Naive Bayes, pipelines integrate random under-sampling to address imbalance, alongside either CountVectorizer or TfidfVectorizer (with unigrams and n-grams).\n",
    "\n",
    "Each pipeline is built using scikit-learnâ€™s `Pipeline` (or `ImbPipeline` for Naive Bayes) to streamline preprocessing, vectorization, sampling, and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kewords:\n",
    "\n",
    "- lr -> Logistic Regression\n",
    "- SVM -> Support vector machine\n",
    "- RF -> Random Forest\n",
    "- NB -> Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Model Pipelines (Binary and TF-IDF, Ngram and Unigram, with and without SVD)\n",
    "\n",
    "\n",
    "# --- Logistic Regression Pipelines (with weighted balancing) ---\n",
    "pipeline_lr_count_unigram = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),\n",
    "    ('vectorizer', CountVectorizer(binary=True, ngram_range=(1,1))),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "pipeline_lr_count_ngram = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),\n",
    "    ('vectorizer', CountVectorizer(binary=True, ngram_range=(1,2))),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "pipeline_lr_tfidf_unigram = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),\n",
    "    ('vectorizer', TfidfVectorizer(ngram_range=(1,1))),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "pipeline_lr_tfidf_ngram = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),\n",
    "    ('vectorizer', TfidfVectorizer(ngram_range=(1,2))),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "\n",
    "# --- SVM Pipelines (using LinearSVC with weighted balancing) ---\n",
    "pipeline_svm_count_unigram = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),\n",
    "    ('vectorizer', CountVectorizer(binary=True, ngram_range=(1,1))),\n",
    "    ('classifier', LinearSVC(max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "pipeline_svm_count_ngram = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),\n",
    "    ('vectorizer', CountVectorizer(binary=True, ngram_range=(1,2))),\n",
    "    ('classifier', LinearSVC(max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "pipeline_svm_tfidf_unigram = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),\n",
    "    ('vectorizer', TfidfVectorizer(ngram_range=(1,1))),\n",
    "    ('classifier', LinearSVC(max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "pipeline_svm_tfidf_ngram = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),\n",
    "    ('vectorizer', TfidfVectorizer(ngram_range=(1,2))),\n",
    "    ('classifier', LinearSVC(max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "\n",
    "# --- Random Forest Pipelines (with weighted balancing) ---\n",
    "pipeline_rf_count_unigram = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),\n",
    "    ('vectorizer', CountVectorizer(binary=True, ngram_range=(1,1))),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "pipeline_rf_count_ngram = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),\n",
    "    ('vectorizer', CountVectorizer(binary=True, ngram_range=(1,2))),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "pipeline_rf_tfidf_unigram = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),\n",
    "    ('vectorizer', TfidfVectorizer(ngram_range=(1,1))),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "pipeline_rf_tfidf_ngram = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),\n",
    "    ('vectorizer', TfidfVectorizer(ngram_range=(1,2))),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "\n",
    "# --- Naive Bayes Pipelines (with Random Under-Sampling) ---\n",
    "pipeline_nb_count_unigram = ImbPipeline([\n",
    "    ('preprocessor', TextPreprocessor()),\n",
    "    ('vectorizer', CountVectorizer(binary=True, ngram_range=(1,1))),\n",
    "    ('sampler', RandomUnderSampler(random_state=42)),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipeline_nb_count_ngram = ImbPipeline([\n",
    "    ('preprocessor', TextPreprocessor()),\n",
    "    ('vectorizer', CountVectorizer(binary=True, ngram_range=(1,2))),\n",
    "    ('sampler', RandomUnderSampler(random_state=42)),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipeline_nb_tfidf_unigram = ImbPipeline([\n",
    "    ('preprocessor', TextPreprocessor()),\n",
    "    ('vectorizer', TfidfVectorizer(ngram_range=(1,1))),\n",
    "    ('sampler', RandomUnderSampler(random_state=42)),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipeline_nb_tfidf_ngram = ImbPipeline([\n",
    "    ('preprocessor', TextPreprocessor()),\n",
    "    ('vectorizer', TfidfVectorizer(ngram_range=(1,2))),\n",
    "    ('sampler', RandomUnderSampler(random_state=42)),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Augmentation and Glove Embeddings\n",
    "\n",
    "\n",
    "1. **Inserting SVD for Dimensionality Reduction:**  \n",
    "   - The helper function `add_svd` takes an existing pipeline and inserts a `TruncatedSVD` step (defaulting to 100 components) right after the vectorizer.  \n",
    "   - This reduces the high-dimensional output from text vectorizers, which can speed up training and potentially improve performance.\n",
    "   - Pipelines that use classifiers other than MultinomialNB (stored in `other_pipelines`) have their SVD versions created using this function. These SVD-augmented pipelines are then merged with the original pipelines (and those for Naive Bayes) into the `all_pipelines` dictionary.\n",
    "\n",
    "2. **Incorporating Glove Embeddings:**  \n",
    "   - Separate pipelines are defined that use the `GloveVectorizer` to convert text into fixed-length embeddings by averaging pre-trained GloVe word vectors (from `glove.twitter.27B.50d.txt`).  \n",
    "   - These pipelines are paired with Logistic Regression, SVM, and Random Forest classifiers.\n",
    "   - The resulting Glove-based pipelines are added to the `all_pipelines` dictionary, enabling comparison with traditional count/Tf-idf based pipelines.\n",
    "\n",
    "Overall, this structure allows you to easily experiment with different vectorization methods (including dimensionality reduction via SVD and Glove embeddings) across various classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to Insert SVD\n",
    "def add_svd(pipeline, n_components=100):\n",
    "    \"\"\"\n",
    "    Inserts a TruncatedSVD step right after the vectorizer.\n",
    "    Assumes the pipeline has steps: preprocessor, vectorizer, classifier.\n",
    "    \"\"\"\n",
    "    steps = pipeline.steps.copy()\n",
    "    # Insert SVD at position 2 (right after vectorizer)\n",
    "    steps.insert(2, ('svd', TruncatedSVD(n_components=n_components)))\n",
    "    return Pipeline(steps)\n",
    "\n",
    "\n",
    "# Create pipelines without SVD for NB\n",
    "pipelines_no_svd = {\n",
    "    # For models that don't use SVD (for NB)\n",
    "    \"NB_Count_Binary_Unigram\": pipeline_nb_count_unigram,\n",
    "    \"NB_Count_Binary_Ngram\": pipeline_nb_count_ngram,\n",
    "    \"NB_Tfidf_Unigram\": pipeline_nb_tfidf_unigram,\n",
    "    \"NB_Tfidf_Ngram\": pipeline_nb_tfidf_ngram,\n",
    "}\n",
    "\n",
    "# For models other than MultinomialNB, with their SVD versions\n",
    "other_pipelines = {\n",
    "    \"LR_Count_Binary_Unigram\": pipeline_lr_count_unigram,\n",
    "    \"LR_Count_Binary_Ngram\": pipeline_lr_count_ngram,\n",
    "    \"LR_Tfidf_Unigram\": pipeline_lr_tfidf_unigram,\n",
    "    \"LR_Tfidf_Ngram\": pipeline_lr_tfidf_ngram,\n",
    "    \n",
    "    \"SVM_Count_Binary_Unigram\": pipeline_svm_count_unigram,\n",
    "    \"SVM_Count_Binary_Ngram\": pipeline_svm_count_ngram,\n",
    "    \"SVM_Tfidf_Unigram\": pipeline_svm_tfidf_unigram,\n",
    "    \"SVM_Tfidf_Ngram\": pipeline_svm_tfidf_ngram,\n",
    "    \n",
    "    \"RF_Count_Binary_Unigram\": pipeline_rf_count_unigram,\n",
    "    \"RF_Count_Binary_Ngram\": pipeline_rf_count_ngram,\n",
    "    \"RF_Tfidf_Unigram\": pipeline_rf_tfidf_unigram,\n",
    "    \"RF_Tfidf_Ngram\": pipeline_rf_tfidf_ngram,\n",
    "}\n",
    "\n",
    "# Create SVD versions for non-NB pipelines\n",
    "svd_pipelines = {name + \"_SVD\": add_svd(pipe) for name, pipe in other_pipelines.items()}\n",
    "\n",
    "# Combine all pipelines\n",
    "all_pipelines = {}\n",
    "all_pipelines.update(pipelines_no_svd)\n",
    "all_pipelines.update(other_pipelines)\n",
    "all_pipelines.update(svd_pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a fixed-length embedding for each document (by averaging word embeddings).\n",
    "pipeline_glove_lr = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),\n",
    "    ('glove', GloveVectorizer(glove_file='glove.twitter.27B.50d.txt', embedding_dim=50)),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "pipeline_glove_svm = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),\n",
    "    ('glove', GloveVectorizer(glove_file='glove.twitter.27B.50d.txt', embedding_dim=50)),\n",
    "    ('classifier', LinearSVC(max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "pipeline_glove_rf = Pipeline([\n",
    "    ('preprocessor', TextPreprocessor()),\n",
    "    ('glove', GloveVectorizer(glove_file='glove.twitter.27B.50d.txt', embedding_dim=50)),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Add the Glove pipelines to all_pipelines dictionary\n",
    "all_pipelines[\"Glove_LR\"] = pipeline_glove_lr\n",
    "all_pipelines[\"Glove_SVM\"] = pipeline_glove_svm\n",
    "all_pipelines[\"Glove_RF\"] = pipeline_glove_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Stage 1 Pipelines\n",
    " \n",
    " We define a helper function that fits a pipeline and returns evaluation metrics:\n",
    " \n",
    " - **Accuracy**  \n",
    " - **Precision** (weighted)  \n",
    " - **Recall** (weighted)  \n",
    " - **F1 Score** (weighted)\n",
    " \n",
    " Then, we loop over all pipelines, evaluate them on the test set, and compile the results into a comparison table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Model Comparison Table\n",
      "                        Pipeline  Accuracy  Precision    Recall  F1 Score\n",
      "0                SVM_Tfidf_Ngram  0.762849   0.742287  0.762849  0.738133\n",
      "1          LR_Count_Binary_Ngram  0.747520   0.728635  0.747520  0.732432\n",
      "2                 LR_Tfidf_Ngram  0.732191   0.729445  0.732191  0.730574\n",
      "3              SVM_Tfidf_Unigram  0.738503   0.725265  0.738503  0.730212\n",
      "4         SVM_Count_Binary_Ngram  0.746619   0.721449  0.746619  0.722890\n",
      "5        LR_Count_Binary_Unigram  0.715059   0.723156  0.715059  0.718613\n",
      "6       SVM_Count_Binary_Unigram  0.712353   0.707297  0.712353  0.709597\n",
      "7               LR_Tfidf_Unigram  0.696123   0.722199  0.696123  0.706270\n",
      "8          SVM_Tfidf_Unigram_SVD  0.699729   0.674637  0.699729  0.682471\n",
      "9                      Glove_SVM  0.689811   0.667190  0.689811  0.675243\n",
      "10  SVM_Count_Binary_Unigram_SVD  0.688909   0.660859  0.688909  0.667307\n",
      "11           SVM_Tfidf_Ngram_SVD  0.687106   0.650704  0.687106  0.659032\n",
      "12    SVM_Count_Binary_Ngram_SVD  0.689811   0.649354  0.689811  0.658068\n",
      "13              RF_Tfidf_Unigram  0.735798   0.773964  0.735798  0.658045\n",
      "14       RF_Count_Binary_Unigram  0.726781   0.717196  0.726781  0.656338\n",
      "15         RF_Count_Binary_Ngram  0.723174   0.738356  0.723174  0.635789\n",
      "16                RF_Tfidf_Ngram  0.724076   0.781881  0.724076  0.630065\n",
      "17          RF_Tfidf_Unigram_SVD  0.709648   0.688937  0.709648  0.613481\n",
      "18            RF_Tfidf_Ngram_SVD  0.710550   0.702921  0.710550  0.608464\n",
      "19                      Glove_RF  0.713255   0.751030  0.713255  0.607063\n",
      "20                      Glove_LR  0.574391   0.710018  0.574391  0.605681\n",
      "21              NB_Tfidf_Unigram  0.569883   0.709158  0.569883  0.602368\n",
      "22     RF_Count_Binary_Ngram_SVD  0.706943   0.661841  0.706943  0.598075\n",
      "23   RF_Count_Binary_Unigram_SVD  0.704238   0.636070  0.704238  0.596024\n",
      "24                NB_Tfidf_Ngram  0.553652   0.696188  0.553652  0.587678\n",
      "25       NB_Count_Binary_Unigram  0.549143   0.699399  0.549143  0.578368\n",
      "26            LR_Tfidf_Ngram_SVD  0.541028   0.681529  0.541028  0.578307\n",
      "27         NB_Count_Binary_Ngram  0.537421   0.698108  0.537421  0.565456\n",
      "28          LR_Tfidf_Unigram_SVD  0.516682   0.665078  0.516682  0.550396\n",
      "29   LR_Count_Binary_Unigram_SVD  0.494139   0.666079  0.494139  0.526683\n",
      "30     LR_Count_Binary_Ngram_SVD  0.477006   0.653126  0.477006  0.513081\n"
     ]
    }
   ],
   "source": [
    "def evaluate_pipeline_metrics(pipeline, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Train the pipeline and return evaluation metrics.\"\"\"\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    predictions = pipeline.predict(X_test)\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_test, predictions),\n",
    "        \"Precision\": precision_score(y_test, predictions, average='weighted', zero_division=0),\n",
    "        \"Recall\": recall_score(y_test, predictions, average='weighted', zero_division=0),\n",
    "        \"F1 Score\": f1_score(y_test, predictions, average='weighted', zero_division=0)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Evaluate each pipeline and store results\n",
    "results = []\n",
    "for name, pipe in all_pipelines.items():\n",
    "    metrics = evaluate_pipeline_metrics(pipe, X_train, X_test, y_train, y_test)\n",
    "    row = {\"Pipeline\": name}\n",
    "    row.update(metrics)\n",
    "    results.append(row)\n",
    "\n",
    "# Create a DataFrame of results and sort by F1 Score\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by=\"F1 Score\", ascending=False)\n",
    "print(\"### Model Comparison Table\")\n",
    "results_df.reset_index(drop=True, inplace=True)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 Analysis of Model Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above presents the performance of 31 different pipelines for a three-class sentiment analysis task, evaluated using weighted metrics. The key metrics reported are Accuracy, Precision, Recall, and F1 Score.\n",
    "\n",
    "## Top Performing Pipelines\n",
    "\n",
    "- **SVM_Tfidf_Ngram (Row 0)**\n",
    "  - **Accuracy:** 0.7628\n",
    "  - **Precision:** 0.7423\n",
    "  - **Recall:** 0.7628\n",
    "  - **F1 Score:** 0.7381\n",
    "  - **Analysis:** This pipeline achieves the highest overall accuracy and consistently high performance across all metrics, making it a strong candidate for sentiment analysis.\n",
    "\n",
    "- **LR_Count_Binary_Ngram (Row 1) and SVM_Count_Binary_Ngram (Row 4)**\n",
    "  - **Accuracy:** ~0.7475 and 0.7466 respectively\n",
    "  - **F1 Score:** ~0.7324 and 0.7229 respectively\n",
    "  - **Analysis:** These pipelines also perform very well, with metrics that are very competitive with the top performer.\n",
    "\n",
    "## Other Notable Pipelines\n",
    "\n",
    "- **LR_Tfidf_Ngram (Row 2) and SVM_Tfidf_Unigram (Row 3)**\n",
    "  - **Accuracy:** 0.7322 and 0.7385 respectively\n",
    "  - **F1 Score:** 0.7306 and 0.7302 respectively\n",
    "  - **Analysis:** These models yield competitive results with a good balance between precision and recall, suggesting robust performance across classes.\n",
    "\n",
    "## Pipelines with Lower Performance\n",
    "\n",
    "- **Pipelines Incorporating SVD:**  \n",
    "  - Examples: SVM_Tfidf_Unigram_SVD (Row 8), SVM_Tfidf_Ngram_SVD (Row 11), LR_Tfidf_Ngram_SVD (Row 26), etc.\n",
    "  - **Observation:** These models generally show a drop in performance (accuracy and F1 Score below 0.70), indicating that dimensionality reduction via SVD may not be beneficial in this setup.\n",
    "\n",
    "- **Pipelines Using Glove Embeddings and Naive Bayes:**\n",
    "  - Examples: Glove_SVM (Row 9), Glove_RF (Row 19), Glove_LR (Row 20), NB_Tfidf_Unigram (Row 21), etc.\n",
    "  - **Observation:** These pipelines exhibit lower F1 Scores (generally in the range of 0.55â€“0.60), suggesting that alternative representations and simpler probabilistic models might be less effective for this task.\n",
    "\n",
    "## Overall Insights\n",
    "\n",
    "- **Best Approach:**  \n",
    "  Traditional pipelines using TF-IDF or CountVectorizer in combination with SVM or Logistic Regression outperform more complex methods involving SVD or Glove embeddings. The top performers maintain a strong balance across all metrics.\n",
    "\n",
    "- **Balanced Performance:**  \n",
    "  The weighted evaluation metrics indicate that the best pipelines are robust across all sentiment classes, with only small differences among the top models. This balanced performance is essential for a multi-class sentiment analysis task.\n",
    "\n",
    "  **Conclusion:**  \n",
    "The analysis suggests that **SVM_Tfidf_Ngram** is the top-performing pipeline based on weighted metrics, with **LR_Count_Binary_Ngram** and **SVM_Count_Binary_Ngram** also showing strong performance. More complex methods involving SVD or alternative embeddings did not outperform these traditional approaches.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________________________________________________________________________________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning the top 5 pipelines, evaluating each on 20% test set (from 80 - 20 split)\n",
    "\n",
    "Pipeline Tuning and Evaluation for Sentiment Analysis\n",
    "\n",
    "This section of the script focuses on tuning and evaluating several pipelines for a three-class sentiment analysis task using GridSearchCV. Each pipeline employs different combinations of text vectorization (using TF-IDF or CountVectorizer) and classification (using SVM or Logistic Regression). The goal is to determine the best hyperparameters for each model and evaluate their performance across multiple metrics.\n",
    "\n",
    "### Steps Followed\n",
    "\n",
    "1. **Define the Parameter Grid:**  \n",
    "   - For each pipeline, we create a dictionary of hyperparameters. \n",
    "2. **Grid Search with Cross-Validation:**  \n",
    "   - We perform grid search using `GridSearchCV` with 5-fold cross-validation.\n",
    "   - The scoring metric used is **F1 macro**, which computes the F1 score for each class and averages them. This is effective in multi-class sentiment analysis, as it treats all classes equally even if the data is balanced.\n",
    "\n",
    "3. **Model Fitting and Best Parameter Selection:**  \n",
    "   - The grid search fits the model on the training data (`X_train` and `y_train`) and selects the best hyperparameters based on the F1 macro score.\n",
    "   - The best parameters are printed for review.\n",
    "\n",
    "4. **Evaluation on Test Data:**  \n",
    "   After selecting the best model, we evaluate its performance on the test set (`X_test`) using multiple metrics:\n",
    "   - **Accuracy**\n",
    "   - **Precision:** Computed as macro, weighted, and micro averages.\n",
    "   - **Recall:** Computed as macro, weighted, and micro averages.\n",
    "   - **F1 Score:** Computed as macro, weighted, and micro averages.\n",
    "\n",
    "5. **Results Storage:**  \n",
    "   The evaluation metrics and best hyperparameters are stored in a global dictionary (`model_results`) for easy comparison across all pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold results for all models\n",
    "model_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for SVM_Tfidf_Ngram:\n",
      "{'classifier__C': 1, 'classifier__loss': 'squared_hinge', 'vectorizer__max_df': 0.9, 'vectorizer__min_df': 2, 'vectorizer__ngram_range': (1, 2), 'vectorizer__use_idf': False}\n",
      "Test Metrics for SVM_Tfidf_Ngram:\n",
      "Accuracy: 0.7448151487826871\n",
      "Precision - Macro: 0.6197368951139476 Weighted: 0.7277096503850062 Micro: 0.7448151487826871\n",
      "Recall    - Macro: 0.5674228818355823 Weighted: 0.7448151487826871 Micro: 0.7448151487826871\n",
      "F1 Score  - Macro: 0.5882174800629737 Weighted: 0.7328421177320352 Micro: 0.7448151487826871\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for SVM_Tfidf_Ngram\n",
    "param_grid = {\n",
    "    'vectorizer__ngram_range': [(1, 2)],\n",
    "    'vectorizer__use_idf': [True, False],\n",
    "    'vectorizer__max_df': [0.9, 1.0],\n",
    "    'vectorizer__min_df': [1, 2],\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__loss': ['hinge', 'squared_hinge']\n",
    "}\n",
    "\n",
    "# Run grid search\n",
    "gs = GridSearchCV(pipeline_svm_tfidf_ngram, param_grid, cv=5, scoring='f1_macro', n_jobs=1, verbose=1)\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Best Parameters for SVM_Tfidf_Ngram:\")\n",
    "print(gs.best_params_)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = gs.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "precision_weighted = precision_score(y_test, y_pred, average='weighted')\n",
    "precision_micro = precision_score(y_test, y_pred, average='micro')\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "recall_weighted = recall_score(y_test, y_pred, average='weighted')\n",
    "recall_micro = recall_score(y_test, y_pred, average='micro')\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "print(\"Test Metrics for SVM_Tfidf_Ngram:\")\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision - Macro:\", precision_macro, \"Weighted:\", precision_weighted, \"Micro:\", precision_micro)\n",
    "print(\"Recall    - Macro:\", recall_macro, \"Weighted:\", recall_weighted, \"Micro:\", recall_micro)\n",
    "print(\"F1 Score  - Macro:\", f1_macro, \"Weighted:\", f1_weighted, \"Micro:\", f1_micro)\n",
    "\n",
    "# Save results\n",
    "model_results['SVM_Tfidf_Ngram'] = {\n",
    "    'best_params': gs.best_params_,\n",
    "    'accuracy': acc,\n",
    "    'precision_macro': precision_macro,\n",
    "    'precision_weighted': precision_weighted,\n",
    "    'precision_micro': precision_micro,\n",
    "    'recall_macro': recall_macro,\n",
    "    'recall_weighted': recall_weighted,\n",
    "    'recall_micro': recall_micro,\n",
    "    'f1_macro': f1_macro,\n",
    "    'f1_weighted': f1_weighted,\n",
    "    'f1_micro': f1_micro\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best Parameters for LR_Count_Binary_Ngram:\n",
      "{'classifier__C': 0.1, 'classifier__class_weight': 'balanced', 'vectorizer__max_df': 0.9, 'vectorizer__min_df': 1, 'vectorizer__ngram_range': (1, 2)}\n",
      "Test Metrics for LR_Count_Binary_Ngram:\n",
      "Accuracy: 0.7123534715960325\n",
      "Precision - Macro: 0.568175326083589 Weighted: 0.7076506422245021 Micro: 0.7123534715960325\n",
      "Recall    - Macro: 0.5618741995393995 Weighted: 0.7123534715960325 Micro: 0.7123534715960325\n",
      "F1 Score  - Macro: 0.5643067712736646 Weighted: 0.7095117507230844 Micro: 0.7123534715960325\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for LR_Count_Binary_Ngram\n",
    "param_grid = {\n",
    "    'vectorizer__ngram_range': [(1, 2)],\n",
    "    'vectorizer__max_df': [0.9, 1.0],\n",
    "    'vectorizer__min_df': [1, 2],\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "# Run grid search\n",
    "gs = GridSearchCV(pipeline_lr_count_ngram, param_grid, cv=5, scoring='f1_macro', n_jobs=1, verbose=1)\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Best Parameters for LR_Count_Binary_Ngram:\")\n",
    "print(gs.best_params_)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = gs.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "precision_weighted = precision_score(y_test, y_pred, average='weighted')\n",
    "precision_micro = precision_score(y_test, y_pred, average='micro')\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "recall_weighted = recall_score(y_test, y_pred, average='weighted')\n",
    "recall_micro = recall_score(y_test, y_pred, average='micro')\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "print(\"Test Metrics for LR_Count_Binary_Ngram:\")\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision - Macro:\", precision_macro, \"Weighted:\", precision_weighted, \"Micro:\", precision_micro)\n",
    "print(\"Recall    - Macro:\", recall_macro, \"Weighted:\", recall_weighted, \"Micro:\", recall_micro)\n",
    "print(\"F1 Score  - Macro:\", f1_macro, \"Weighted:\", f1_weighted, \"Micro:\", f1_micro)\n",
    "\n",
    "# Save results\n",
    "model_results['LR_Count_Binary_Ngram'] = {\n",
    "    'best_params': gs.best_params_,\n",
    "    'accuracy': acc,\n",
    "    'precision_macro': precision_macro,\n",
    "    'precision_weighted': precision_weighted,\n",
    "    'precision_micro': precision_micro,\n",
    "    'recall_macro': recall_macro,\n",
    "    'recall_weighted': recall_weighted,\n",
    "    'recall_micro': recall_micro,\n",
    "    'f1_macro': f1_macro,\n",
    "    'f1_weighted': f1_weighted,\n",
    "    'f1_micro': f1_micro\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Parameters for LR_Tfidf_Ngram:\n",
      "{'classifier__C': 1, 'classifier__class_weight': 'balanced', 'vectorizer__ngram_range': (1, 2), 'vectorizer__use_idf': True}\n",
      "Test Metrics for LR_Tfidf_Ngram:\n",
      "Accuracy: 0.7321911632100991\n",
      "Precision - Macro: 0.600318552601035 Weighted: 0.7294454546067841 Micro: 0.7321911632100991\n",
      "Recall    - Macro: 0.5977719095294677 Weighted: 0.7321911632100991 Micro: 0.7321911632100991\n",
      "F1 Score  - Macro: 0.5986477334344934 Weighted: 0.7305738680821262 Micro: 0.7321911632100991\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for LR_Tfidf_Ngram\n",
    "param_grid = {\n",
    "    'vectorizer__ngram_range': [(1, 2)],\n",
    "    'vectorizer__use_idf': [True, False],\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "# Run grid search\n",
    "gs = GridSearchCV(pipeline_lr_tfidf_ngram, param_grid, cv=5, scoring='f1_macro', n_jobs=1, verbose=1)\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Best Parameters for LR_Tfidf_Ngram:\")\n",
    "print(gs.best_params_)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = gs.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "precision_weighted = precision_score(y_test, y_pred, average='weighted')\n",
    "precision_micro = precision_score(y_test, y_pred, average='micro')\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "recall_weighted = recall_score(y_test, y_pred, average='weighted')\n",
    "recall_micro = recall_score(y_test, y_pred, average='micro')\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "print(\"Test Metrics for LR_Tfidf_Ngram:\")\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision - Macro:\", precision_macro, \"Weighted:\", precision_weighted, \"Micro:\", precision_micro)\n",
    "print(\"Recall    - Macro:\", recall_macro, \"Weighted:\", recall_weighted, \"Micro:\", recall_micro)\n",
    "print(\"F1 Score  - Macro:\", f1_macro, \"Weighted:\", f1_weighted, \"Micro:\", f1_micro)\n",
    "\n",
    "# Save results\n",
    "model_results['LR_Tfidf_Ngram'] = {\n",
    "    'best_params': gs.best_params_,\n",
    "    'accuracy': acc,\n",
    "    'precision_macro': precision_macro,\n",
    "    'precision_weighted': precision_weighted,\n",
    "    'precision_micro': precision_micro,\n",
    "    'recall_macro': recall_macro,\n",
    "    'recall_weighted': recall_weighted,\n",
    "    'recall_micro': recall_micro,\n",
    "    'f1_macro': f1_macro,\n",
    "    'f1_weighted': f1_weighted,\n",
    "    'f1_micro': f1_micro\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for SVM_Tfidf_Unigram:\n",
      "{'classifier__C': 1, 'classifier__class_weight': 'balanced', 'classifier__loss': 'squared_hinge', 'vectorizer__max_df': 0.9, 'vectorizer__min_df': 2, 'vectorizer__ngram_range': (1, 1), 'vectorizer__use_idf': False}\n",
      "Test Metrics for SVM_Tfidf_Unigram:\n",
      "Accuracy: 0.7276825969341749\n",
      "Precision - Macro: 0.59049139842036 Weighted: 0.7173221303949213 Micro: 0.7276825969341749\n",
      "Recall    - Macro: 0.5664867359537832 Weighted: 0.7276825969341749 Micro: 0.7276825969341749\n",
      "F1 Score  - Macro: 0.5769678402386047 Weighted: 0.7213548930090299 Micro: 0.7276825969341749\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for SVM_Tfidf_Unigram\n",
    "param_grid = {\n",
    "    'vectorizer__ngram_range': [(1, 1)],\n",
    "    'vectorizer__use_idf': [True, False],\n",
    "    'vectorizer__max_df': [0.9, 1.0],\n",
    "    'vectorizer__min_df': [1, 2],\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__loss': ['hinge', 'squared_hinge'],\n",
    "    'classifier__class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "# Run grid search\n",
    "gs = GridSearchCV(pipeline_svm_tfidf_unigram, param_grid, cv=5, scoring='f1_macro', n_jobs=1, verbose=1)\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Best Parameters for SVM_Tfidf_Unigram:\")\n",
    "print(gs.best_params_)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = gs.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "precision_weighted = precision_score(y_test, y_pred, average='weighted')\n",
    "precision_micro = precision_score(y_test, y_pred, average='micro')\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "recall_weighted = recall_score(y_test, y_pred, average='weighted')\n",
    "recall_micro = recall_score(y_test, y_pred, average='micro')\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "print(\"Test Metrics for SVM_Tfidf_Unigram:\")\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision - Macro:\", precision_macro, \"Weighted:\", precision_weighted, \"Micro:\", precision_micro)\n",
    "print(\"Recall    - Macro:\", recall_macro, \"Weighted:\", recall_weighted, \"Micro:\", recall_micro)\n",
    "print(\"F1 Score  - Macro:\", f1_macro, \"Weighted:\", f1_weighted, \"Micro:\", f1_micro)\n",
    "\n",
    "# Save results\n",
    "model_results['SVM_Tfidf_Unigram'] = {\n",
    "    'best_params': gs.best_params_,\n",
    "    'accuracy': acc,\n",
    "    'precision_macro': precision_macro,\n",
    "    'precision_weighted': precision_weighted,\n",
    "    'precision_micro': precision_micro,\n",
    "    'recall_macro': recall_macro,\n",
    "    'recall_weighted': recall_weighted,\n",
    "    'recall_micro': recall_micro,\n",
    "    'f1_macro': f1_macro,\n",
    "    'f1_weighted': f1_weighted,\n",
    "    'f1_micro': f1_micro\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for SVM_Count_Binary_Ngram:\n",
      "{'classifier__C': 0.1, 'classifier__class_weight': 'balanced', 'classifier__loss': 'squared_hinge', 'vectorizer__max_df': 0.9, 'vectorizer__min_df': 2, 'vectorizer__ngram_range': (1, 2)}\n",
      "Test Metrics for SVM_Count_Binary_Ngram:\n",
      "Accuracy: 0.7385031559963932\n",
      "Precision - Macro: 0.6149719615790351 Weighted: 0.7186568422593567 Micro: 0.7385031559963932\n",
      "Recall    - Macro: 0.5514887912714003 Weighted: 0.7385031559963932 Micro: 0.7385031559963932\n",
      "F1 Score  - Macro: 0.575187325515886 Weighted: 0.7234374944469549 Micro: 0.7385031559963932\n"
     ]
    }
   ],
   "source": [
    "# Define parameter grid for SVM_Count_Binary_Ngram\n",
    "param_grid = {\n",
    "    'vectorizer__ngram_range': [(1, 2)],\n",
    "    'vectorizer__max_df': [0.9, 1.0],\n",
    "    'vectorizer__min_df': [1, 2],\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__loss': ['hinge', 'squared_hinge'],\n",
    "    'classifier__class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "# Run grid search\n",
    "gs = GridSearchCV(pipeline_svm_count_ngram, param_grid, cv=5, scoring='f1_macro', n_jobs=1, verbose=1)\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"Best Parameters for SVM_Count_Binary_Ngram:\")\n",
    "print(gs.best_params_)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = gs.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "precision_weighted = precision_score(y_test, y_pred, average='weighted')\n",
    "precision_micro = precision_score(y_test, y_pred, average='micro')\n",
    "recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "recall_weighted = recall_score(y_test, y_pred, average='weighted')\n",
    "recall_micro = recall_score(y_test, y_pred, average='micro')\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "print(\"Test Metrics for SVM_Count_Binary_Ngram:\")\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision - Macro:\", precision_macro, \"Weighted:\", precision_weighted, \"Micro:\", precision_micro)\n",
    "print(\"Recall    - Macro:\", recall_macro, \"Weighted:\", recall_weighted, \"Micro:\", recall_micro)\n",
    "print(\"F1 Score  - Macro:\", f1_macro, \"Weighted:\", f1_weighted, \"Micro:\", f1_micro)\n",
    "\n",
    "# Save results\n",
    "model_results['SVM_Count_Binary_Ngram'] = {\n",
    "    'best_params': gs.best_params_,\n",
    "    'accuracy': acc,\n",
    "    'precision_macro': precision_macro,\n",
    "    'precision_weighted': precision_weighted,\n",
    "    'precision_micro': precision_micro,\n",
    "    'recall_macro': recall_macro,\n",
    "    'recall_weighted': recall_weighted,\n",
    "    'recall_micro': recall_micro,\n",
    "    'f1_macro': f1_macro,\n",
    "    'f1_weighted': f1_weighted,\n",
    "    'f1_micro': f1_micro\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. f1_macro calculates the F1 score for each class independently and averages them, ensuring that all classes contribute equally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Model                                        best_params  \\\n",
      "0         SVM_Tfidf_Ngram  {'classifier__C': 1, 'classifier__loss': 'squa...   \n",
      "1   LR_Count_Binary_Ngram  {'classifier__C': 0.1, 'classifier__class_weig...   \n",
      "2          LR_Tfidf_Ngram  {'classifier__C': 1, 'classifier__class_weight...   \n",
      "3       SVM_Tfidf_Unigram  {'classifier__C': 1, 'classifier__class_weight...   \n",
      "4  SVM_Count_Binary_Ngram  {'classifier__C': 0.1, 'classifier__class_weig...   \n",
      "\n",
      "  accuracy precision_macro precision_weighted precision_micro recall_macro  \\\n",
      "0   0.7448          0.6197             0.7277          0.7448       0.5674   \n",
      "1   0.7124          0.5682             0.7077          0.7124       0.5619   \n",
      "2   0.7322          0.6003             0.7294          0.7322       0.5978   \n",
      "3   0.7277          0.5905             0.7173          0.7277       0.5665   \n",
      "4   0.7385          0.6150             0.7187          0.7385       0.5515   \n",
      "\n",
      "  recall_weighted recall_micro f1_macro f1_weighted f1_micro  \n",
      "0          0.7448       0.7448   0.5882      0.7328   0.7448  \n",
      "1          0.7124       0.7124   0.5643      0.7095   0.7124  \n",
      "2          0.7322       0.7322   0.5986      0.7306   0.7322  \n",
      "3          0.7277       0.7277   0.5770      0.7214   0.7277  \n",
      "4          0.7385       0.7385   0.5752      0.7234   0.7385  \n"
     ]
    }
   ],
   "source": [
    "# Convert the model_results dictionary into a DataFrame for easy comparison\n",
    "results_df = pd.DataFrame(model_results).T.reset_index().rename(columns={'index': 'Model'})\n",
    "\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 Analysis of Model Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Observations\n",
    "\n",
    "- **Overall Accuracy:**\n",
    "  - The **SVM_Tfidf_Ngram** pipeline achieved the highest accuracy (0.7448), suggesting it predicts the correct class more often than the others.\n",
    "\n",
    "- **F1 Macro Score:**\n",
    "  - The **LR_Tfidf_Ngram** pipeline shows the highest macro F1 score (0.5986), indicating a better balance between precision and recall across all classes.\n",
    "  - The macro F1 score is important in multi-class sentiment analysis as it treats each class equally, regardless of class frequencies.\n",
    "\n",
    "- **Precision and Recall:**\n",
    "  - While **SVM_Count_Binary_Ngram** has a relatively high macro precision (0.6150), its macro recall (0.5515) is lower compared to the others. This suggests it is more conservativeâ€”fewer false positives but possibly missing more true positives.\n",
    "  - **LR_Count_Binary_Ngram** and **SVM_Tfidf_Unigram** show similar performance patterns with moderate precision and recall, leading to comparable macro F1 scores (around 0.56 to 0.58).\n",
    "\n",
    "- **Weighted Averages:**\n",
    "  - The weighted metrics (which account for class frequency) are quite similar across all pipelines, with F1 weighted scores ranging from approximately 0.7095 to 0.7328. This consistency reflects the balanced nature of the dataset.\n",
    "  \n",
    "## Conclusion\n",
    "\n",
    "Each model has its own strengths:\n",
    "- **SVM_Tfidf_Ngram** leads in overall accuracy.\n",
    "- **LR_Tfidf_Ngram** achieves the best balance across classes as reflected in its higher macro F1 score.\n",
    "- The differences among the models are relatively small, so the final model selection might depend on whether you prioritize overall accuracy or balanced class performance.\n",
    "\n",
    "This comprehensive evaluation helps in understanding the trade-offs between different pipelines, ensuring that the final choice aligns with the specific requirements of the sentiment analysis task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
