{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Combiner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The role of this section is to combine the all extracted data from all the different folders into a single csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the data across different folders and queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reterive_data(folder_path):\n",
    "    parent_folder =  folder_path\n",
    "\n",
    "    # List to collect all dataframes\n",
    "    all_dfs = []\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Walk through parent_folder and find subfolders containing data_<x>.csv & query.txt\n",
    "    for root, dirs, files in os.walk(parent_folder):\n",
    "        \n",
    "\n",
    "        csv_files = [f for f in files if f.lower().endswith(\".csv\")]\n",
    "        txt_files = [f for f in files if f.lower().endswith(\".txt\")]\n",
    "\n",
    "        # We expect exactly one CSV (e.g. data_<x>.csv) and one query.txt in each subfolder\n",
    "        if len(csv_files) == 1 and len(txt_files) == 1:\n",
    "\n",
    "            # print(f\"Processing folder: {root}...\")\n",
    "\n",
    "            csv_path = os.path.join(root, csv_files[0])\n",
    "            txt_path = os.path.join(root, txt_files[0])\n",
    "\n",
    "            # Read the query from the query.txt file\n",
    "            with open(txt_path, \"r\", encoding=\"utf-8\") as txt_file:\n",
    "                query_text = txt_file.read().strip()\n",
    "\n",
    "            # Read the CSV into a DataFrame\n",
    "            df = pd.read_csv(csv_path)\n",
    "\n",
    "            # Append the query text as a new column\n",
    "            df[\"query\"] = query_text\n",
    "\n",
    "            # Collect for later concatenation\n",
    "            all_dfs.append(df)\n",
    "\n",
    "    if not all_dfs:\n",
    "        print(\"No valid CSV/query pairs found. Exiting.\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    # print(\"=\"*50)\n",
    "\n",
    "    # Concatenate all dataframes\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "    # Drop duplicates based on the composite key (post_id, comment_id)\n",
    "    combined_df.drop_duplicates(subset=[\"post_id\", \"comment_id\"], inplace=True)\n",
    "\n",
    "    # Reorder columns to match the final desired format\n",
    "    final_columns = [\n",
    "        \"post_id\",\n",
    "        \"comment_id\",\n",
    "        \"title\",\n",
    "        \"body\",\n",
    "        \"subreddit\",\n",
    "        \"upvotes\",\n",
    "        \"comments\",\n",
    "        \"date_time\",\n",
    "        \"author\",\n",
    "        \"query\"\n",
    "    ]\n",
    "    # Keep only these columns (if they exist) in the correct order\n",
    "    # (In case some CSV might have extra columns, or missing columns raise error)\n",
    "    existing_columns = [col for col in final_columns if col in combined_df.columns]\n",
    "    combined_df = combined_df[existing_columns]\n",
    "\n",
    "    # Print no.of posts and comments\n",
    "    num_of_posts = combined_df[\"post_id\"].nunique()\n",
    "    num_of_comments = combined_df[\"comment_id\"].nunique()\n",
    "    print(f\"Total number of posts: {num_of_posts}\")\n",
    "    print(f\"Total number of comments: {num_of_comments}\")\n",
    "    print(f\"Total number of Records: {num_of_posts + num_of_comments:,}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Write combined data to CSV\n",
    "    output_csv = \"combined_data.csv\"\n",
    "    combined_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Combined CSV successfully saved as {output_csv}\")\n",
    "    print(f\"File size: {os.path.getsize(output_csv)/(1024*1024):.2f} MB\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Total number of posts: 3879\n",
      "Total number of comments: 289348\n",
      "Total number of Records: 293,227\n",
      "==================================================\n",
      "Combined CSV successfully saved as combined_data.csv\n",
      "File size: 112.34 MB\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "reterive_data(raw_data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
