{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts: 9575\n",
      "Posts without comments: 614\n",
      "\n",
      "After cleaning the data\n",
      "Number of posts: 8513\n",
      "Posts without comments: 550\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('new_combined_dataset.csv')\n",
    "\n",
    "print(\"Number of posts:\",len(data['post_id'].unique()))\n",
    "print(\"Posts without comments:\", len(data[data['comment_id'].isna()]))\n",
    "\n",
    "\n",
    "\n",
    "# Ensure that the post_id is 7 characters long\n",
    "data = data[data['post_id'].str.len() == 7]\n",
    "\n",
    "#Remove all records where subreddit is null\n",
    "data = data[data['subreddit'].notnull()]\n",
    "\n",
    "\n",
    "data['post_title'] = data['post_title'].str.replace('\\n', ' ')\n",
    "data['post_body'] = data['post_body'].str.replace('\\n', ' ')\n",
    "data['comment_body'] = data['comment_body'].str.replace('\\n', ' ')\n",
    "\n",
    "# Reencode the data to utf-8\n",
    "data['post_title'] = data['post_title'].str.encode('utf-8', 'ignore').str.decode('utf-8')\n",
    "data['post_body'] = data['post_body'].str.encode('utf-8', 'ignore').str.decode('utf-8')\n",
    "data['comment_body'] = data['comment_body'].str.encode('utf-8', 'ignore').str.decode('utf-8')\n",
    "\n",
    "data['number_of_upvotes'] = data['number_of_upvotes'].fillna(0)\n",
    "\n",
    "# Remove where comment_body is [deleted] or [removed]\n",
    "data = data[data['comment_body'] != '[deleted]']\n",
    "data = data[data['comment_body'] != '[removed]']\n",
    "\n",
    "# Remove comment body when count is greater than 50\n",
    "# Keep records where comment_body is null or appears <= 50 times\n",
    "# Create a mask for records where comment_body is null or appears <= 50 times\n",
    "comment_counts = data['comment_body'].value_counts()\n",
    "mask = (data['comment_body'].isna()) | (data['comment_body'].map(lambda x: comment_counts.get(x, 0) <= 50))\n",
    "data = data[mask]\n",
    "\n",
    "# Remove data that matches the regex pattern, handling NaN values\n",
    "data = data[~data['comment_body'].fillna('').str.contains(r'^Hey\\s+/u/\\w+.*?$', regex=True)]\n",
    "data = data[~data['comment_body'].fillna('').str.contains(r'^.*?if you have any questions or concerns.*?$', regex=True)]\n",
    "data = data[~data['comment_body'].fillna('').str.contains(r'\\[ Removed by Reddit \\]', regex=True)]\n",
    "data = data[~data['comment_body'].fillna('').str.contains(r'^.*?\\[.*?\\].*?$', regex=True)]\n",
    "\n",
    "# Store the data in a new CSV file\n",
    "data.to_csv('cleaned_data.csv', index=False)\n",
    "\n",
    "\n",
    "# Check number post (has post_id but no comment_id)\n",
    "print(\"\\nAfter cleaning the data\")\n",
    "print(\"Number of posts:\",len(data['post_id'].unique()))\n",
    "print(\"Posts without comments:\", len(data[data['comment_id'].isna()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   post_id   subreddit                                        post_title  \\\n",
      "0  1002dom  technology  ChatGPT Caused 'Code Red' at Google, Report Says   \n",
      "1  1002dom  technology  ChatGPT Caused 'Code Red' at Google, Report Says   \n",
      "2  1002dom  technology  ChatGPT Caused 'Code Red' at Google, Report Says   \n",
      "3  1002dom  technology  ChatGPT Caused 'Code Red' at Google, Report Says   \n",
      "4  1002dom  technology  ChatGPT Caused 'Code Red' at Google, Report Says   \n",
      "\n",
      "  post_body  number_of_comments   readable_datetime post_author comment_id  \\\n",
      "0      None                 370 2023-01-01 00:03:33    slakmehl    j2far1e   \n",
      "1      None                 370 2023-01-01 00:03:33    slakmehl    j2f5vg2   \n",
      "2      None                 370 2023-01-01 00:03:33    slakmehl    j2f9y5m   \n",
      "3      None                 370 2023-01-01 00:03:33    slakmehl    j2f7njc   \n",
      "4      None                 370 2023-01-01 00:03:33    slakmehl    j2fna2c   \n",
      "\n",
      "                                        comment_body  number_of_upvotes  \\\n",
      "0                    Chat GPT wrote this article ffs                792   \n",
      "1                        Did you order the code red?                687   \n",
      "2  If my search engine was littered with SEO keyw...               1288   \n",
      "3    How many more times are we gonna see this story                306   \n",
      "4  How far can we trust ChatGPT? It's very intere...                 70   \n",
      "\n",
      "        comment_author    query  \n",
      "0  The_Bridge_Imperium  ChatGPT  \n",
      "1            damienn22  ChatGPT  \n",
      "2              1x2x4x1  ChatGPT  \n",
      "3            frombaktk  ChatGPT  \n",
      "4         Milk_Busters  ChatGPT  \n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_csv('cleaned_data.csv', \n",
    "\t\t\t\t\t   engine='pyarrow',     # Use python engine instead of pyarrow\n",
    "\t\t\t\t\t   encoding='utf-8',    # Specify encoding\n",
    ")\n",
    "\n",
    "# Declare each field data type\n",
    "raw_data['post_id'] = raw_data['post_id'].astype(str)\n",
    "raw_data['comment_id'] = raw_data['comment_id'].astype(str)\n",
    "raw_data['post_title'] = raw_data['post_title'].astype(str)\n",
    "raw_data['post_body'] = raw_data['post_body'].astype(str)\n",
    "raw_data['post_author'] = raw_data['post_author'].astype(str)\n",
    "raw_data['comment_body'] = raw_data['comment_body'].astype(str)\n",
    "raw_data['comment_author'] = raw_data['comment_author'].astype(str)\n",
    "raw_data['query'] = raw_data['query'].astype(str)\n",
    "\n",
    "raw_data['subreddit'] = raw_data['subreddit'].astype('category')\n",
    "raw_data['query'] = raw_data['query'].astype('category')\n",
    "\n",
    "# Fill NaN values with 0 before converting to int\n",
    "raw_data['number_of_comments'] = raw_data['number_of_comments'].fillna(0).astype(int)\n",
    "raw_data['number_of_upvotes'] = raw_data['number_of_upvotes'].fillna(0).astype(int)\n",
    "\n",
    "raw_data['readable_datetime'] = pd.to_datetime(raw_data['readable_datetime'])\n",
    "\n",
    "print(raw_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty texts: 0\n",
      "Number of NA texts: 0\n",
      "Total records: 46901\n",
      "\n",
      "Number of posts: 8513\n",
      "\n",
      " After adding new row for all unique posts\n",
      "\n",
      "Number of empty texts: 0\n",
      "Number of NA texts: 0\n",
      "Total records: 55414\n"
     ]
    }
   ],
   "source": [
    "# Create combined text field and replace None/NaN with empty string\n",
    "raw_data[\"text\"] = raw_data.apply(\n",
    "\tlambda row: (\n",
    "\t\tstr(row['comment_body']).strip() if pd.notna(row['comment_body'])\n",
    "\t\telse \"\"), \n",
    "\taxis=1\n",
    ").fillna(\"\")\n",
    "\n",
    "# Print count\n",
    "print(\"Number of empty texts:\", (raw_data[\"text\"] == \"\").sum())\n",
    "print(\"Number of NA texts:\", raw_data[\"text\"].isna().sum())\n",
    "print(\"Total records:\", len(raw_data))\n",
    "\n",
    "# Drop rows where text is empty\n",
    "raw_data = raw_data[raw_data[\"text\"] != \"\"]\n",
    "\n",
    "# Add a new row for all unique posts (get post_id, comments, and all from first record of the post, exclude any comment fields)\n",
    "posts = raw_data.groupby(\"post_id\").first().reset_index()\n",
    "posts = posts.drop(columns=[\"comment_id\", \"comment_body\", \"comment_author\",\"text\"])\n",
    "\n",
    "print(\"\\nNumber of posts:\", len(posts))\n",
    "\n",
    "# Make post text the as post title and post body\n",
    "posts[\"text\"] = posts[\"post_title\"] + \" \" + posts[\"post_body\"]\n",
    "\n",
    "# concat raw data and posts\n",
    "raw_data = pd.concat([posts, raw_data], ignore_index=True)\n",
    "\n",
    "# Remove any duplicate rows\n",
    "raw_data = raw_data.drop_duplicates()\n",
    "\n",
    "# Display first few rows and value counts of empty strings\n",
    "print(\"\\n After adding new row for all unique posts\")\n",
    "print(\"\\nNumber of empty texts:\", (raw_data[\"text\"] == \"\").sum())\n",
    "print(\"Number of NA texts:\", raw_data[\"text\"].isna().sum())\n",
    "print(\"Total records:\", len(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Posts and Comments:\n",
      "8513\n",
      "46901\n"
     ]
    }
   ],
   "source": [
    "# No.of Posts and Comments\n",
    "print(\"\\nNumber of Posts and Comments:\")\n",
    "print(raw_data[\"post_id\"].nunique())\n",
    "print(raw_data[\"comment_id\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = raw_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  S1: Filter to past 5 year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Posts and Comments after filtering:\n",
      "8513\n",
      "46901\n",
      "\n",
      "Date time range:\n",
      "2023-01-01 00:03:33\n",
      "2025-01-30 12:00:03\n"
     ]
    }
   ],
   "source": [
    "cutoff_date = datetime.now() - timedelta(days=5*365)\n",
    "\n",
    "filtered_data = filtered_data[filtered_data[\"readable_datetime\"] > cutoff_date]\n",
    "\n",
    "print(\"\\nNumber of Posts and Comments after filtering:\")\n",
    "print(filtered_data[\"post_id\"].nunique())\n",
    "print(filtered_data[\"comment_id\"].count())\n",
    "\n",
    "# Print date time range in the data\n",
    "print(\"\\nDate time range:\")\n",
    "print(filtered_data[\"readable_datetime\"].min())\n",
    "print(filtered_data[\"readable_datetime\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all missing records where text is empty\n",
    "filtered_data = filtered_data[filtered_data[\"text\"] != \"\"]\n",
    "filtered_data = filtered_data[filtered_data[\"text\"].notna()]\n",
    "filtered_data = filtered_data[filtered_data[\"text\"] != \"nan\"]\n",
    "filtered_data = filtered_data[filtered_data[\"text\"] != \"None\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2: Text Length\n",
    "\n",
    "Short Texts: Extremely short texts (e.g., those with only one or two words) might not provide enough context and could be noise.\n",
    "\n",
    "Excessively Long Texts: Conversely, texts that far exceed the typical length for your domain might be off-topic or contain noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word count statistics:\n",
      "  count     mean      std    min    25%    50%    75%    max\n",
      "-------  -------  -------  -----  -----  -----  -----  -----\n",
      "  54862  51.6563  135.786      1     10     21     49   5827\n",
      "\n",
      "\n",
      "Max words set to: 107\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "min_words = 3\n",
    "\n",
    "# Calculate word counts for each text\n",
    "word_counts = filtered_data['text'].str.split().str.len()\n",
    "\n",
    "print(\"\\nWord count statistics:\")\n",
    "print(tabulate([word_counts.describe()], headers='keys'))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Set max_words as the upper quartile (75th percentile) plus 1.5 times IQR\n",
    "Q3 = word_counts.quantile(0.75)\n",
    "Q1 = word_counts.quantile(0.25)\n",
    "IQR = Q3 - Q1\n",
    "max_words = int(Q3 + 1.5 * IQR)\n",
    "\n",
    "print(f\"Max words set to: {max_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8513\n",
      "8513\n",
      "\n",
      "Number of Posts and Comments after filtering by word count:\n",
      "8267\n",
      "5978\n"
     ]
    }
   ],
   "source": [
    "print(filtered_data[\"post_id\"].nunique())\n",
    "print(filtered_data[\"comment_id\"].isna().sum())\n",
    "\n",
    "# First recalculate word counts since filtered_data has been modified since last count\n",
    "word_counts = filtered_data['text'].str.split().str.len()\n",
    "\n",
    "# Filter based on min and max words\n",
    "filtered_data = filtered_data[word_counts.between(min_words, max_words)]\n",
    "\n",
    "print(\"\\nNumber of Posts and Comments after filtering by word count:\")\n",
    "print(filtered_data[\"post_id\"].nunique())\n",
    "print(filtered_data[\"comment_id\"].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the filtered data in a new CSV file\n",
    "filtered_data.to_csv('filtered_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
