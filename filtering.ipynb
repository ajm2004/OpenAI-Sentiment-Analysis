{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('new_combined_dataset.csv')\n",
    "\n",
    "# Ensure that the post_id is 7 characters long\n",
    "data = data[data['post_id'].str.len() == 7]\n",
    "\n",
    "#Remove all records where subreddit is null\n",
    "data = data[data['subreddit'].notnull()]\n",
    "\n",
    "\n",
    "data['post_title'] = data['post_title'].str.replace('\\n', ' ')\n",
    "data['post_body'] = data['post_body'].str.replace('\\n', ' ')\n",
    "data['comment_body'] = data['comment_body'].str.replace('\\n', ' ')\n",
    "\n",
    "# Reencode the data to utf-8\n",
    "data['post_body'] = data['post_body'].str.encode('utf-8', 'ignore').str.decode('utf-8')\n",
    "data['comment_body'] = data['comment_body'].str.encode('utf-8', 'ignore').str.decode('utf-8')\n",
    "\n",
    "data['number_of_upvotes'] = data['number_of_upvotes'].fillna(0)\n",
    "\n",
    "# Remove where comment_body is [deleted] or [removed]\n",
    "data = data[data['comment_body'] != '[deleted]']\n",
    "data = data[data['comment_body'] != '[removed]']\n",
    "\n",
    "# Remove comment body when count is greater than 50\n",
    "data = data.groupby('comment_body').filter(lambda x: len(x) <= 50)\n",
    "\n",
    "# Remove data that matches the regex pattern\n",
    "data = data[~data['comment_body'].str.contains(r'^Hey\\s+/u/\\w+.*?$', regex=True)]\n",
    "data = data[~data['comment_body'].str.contains(r'^.*?if you have any questions or concerns.*?$', regex=True)]\n",
    "data = data[~data['comment_body'].str.contains(r'\\[ Removed by Reddit \\]', regex=True)]\n",
    "data = data[~data['comment_body'].str.contains(r'^.*?\\[.*?\\].*?$', regex=True)]\n",
    "\n",
    "# Store the data in a new CSV file\n",
    "data.to_csv('cleaned_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   post_id   subreddit                                        post_title  \\\n",
      "0  1002dom  technology  ChatGPT Caused 'Code Red' at Google, Report Says   \n",
      "1  1002dom  technology  ChatGPT Caused 'Code Red' at Google, Report Says   \n",
      "2  1002dom  technology  ChatGPT Caused 'Code Red' at Google, Report Says   \n",
      "3  1002dom  technology  ChatGPT Caused 'Code Red' at Google, Report Says   \n",
      "4  1002dom  technology  ChatGPT Caused 'Code Red' at Google, Report Says   \n",
      "\n",
      "  post_body  number_of_comments   readable_datetime post_author comment_id  \\\n",
      "0      None                 370 2023-01-01 00:03:33    slakmehl    j2far1e   \n",
      "1      None                 370 2023-01-01 00:03:33    slakmehl    j2f5vg2   \n",
      "2      None                 370 2023-01-01 00:03:33    slakmehl    j2f9y5m   \n",
      "3      None                 370 2023-01-01 00:03:33    slakmehl    j2f7njc   \n",
      "4      None                 370 2023-01-01 00:03:33    slakmehl    j2fna2c   \n",
      "\n",
      "                                        comment_body  number_of_upvotes  \\\n",
      "0                    Chat GPT wrote this article ffs                792   \n",
      "1                        Did you order the code red?                687   \n",
      "2  If my search engine was littered with SEO keyw...               1288   \n",
      "3    How many more times are we gonna see this story                306   \n",
      "4  How far can we trust ChatGPT? It's very intere...                 70   \n",
      "\n",
      "        comment_author    query  \n",
      "0  The_Bridge_Imperium  ChatGPT  \n",
      "1            damienn22  ChatGPT  \n",
      "2              1x2x4x1  ChatGPT  \n",
      "3            frombaktk  ChatGPT  \n",
      "4         Milk_Busters  ChatGPT  \n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_csv('cleaned_data.csv', \n",
    "\t\t\t\t\t   engine='pyarrow',     # Use python engine instead of pyarrow\n",
    "\t\t\t\t\t   encoding='utf-8',    # Specify encoding\n",
    ")\n",
    "\n",
    "# Declare each field data type\n",
    "raw_data['post_id'] = raw_data['post_id'].astype(str)\n",
    "raw_data['comment_id'] = raw_data['comment_id'].astype(str)\n",
    "raw_data['post_title'] = raw_data['post_title'].astype(str)\n",
    "raw_data['post_body'] = raw_data['post_body'].astype(str)\n",
    "raw_data['post_author'] = raw_data['post_author'].astype(str)\n",
    "raw_data['comment_body'] = raw_data['comment_body'].astype(str)\n",
    "raw_data['comment_author'] = raw_data['comment_author'].astype(str)\n",
    "raw_data['query'] = raw_data['query'].astype(str)\n",
    "\n",
    "raw_data['subreddit'] = raw_data['subreddit'].astype('category')\n",
    "raw_data['query'] = raw_data['query'].astype('category')\n",
    "\n",
    "# Fill NaN values with 0 before converting to int\n",
    "raw_data['number_of_comments'] = raw_data['number_of_comments'].fillna(0).astype(int)\n",
    "raw_data['number_of_upvotes'] = raw_data['number_of_upvotes'].fillna(0).astype(int)\n",
    "\n",
    "raw_data['readable_datetime'] = pd.to_datetime(raw_data['readable_datetime'])\n",
    "\n",
    "print(raw_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample texts:\n",
      "0    ChatGPT Caused 'Code Red' at Google, Report Sa...\n",
      "1    ChatGPT Caused 'Code Red' at Google, Report Sa...\n",
      "2    ChatGPT Caused 'Code Red' at Google, Report Sa...\n",
      "3    ChatGPT Caused 'Code Red' at Google, Report Sa...\n",
      "4    ChatGPT Caused 'Code Red' at Google, Report Sa...\n",
      "Name: text, dtype: object\n",
      "\n",
      "Number of empty texts: 0\n"
     ]
    }
   ],
   "source": [
    "# Create combined text field and replace None/NaN with empty string\n",
    "raw_data[\"text\"] = raw_data.apply(\n",
    "\tlambda row: (\n",
    "\t\t\t\tf\"{str(row['comment_body']).strip()}\" if pd.notna(row['comment_body'])\n",
    "\t\t\t\telse f\"{str(row['post_title']).strip()} {str(row['post_body']).strip()}\" if pd.notna(row['post_title']) and pd.notna(row['post_body'])\n",
    "\t\t\t\telse str(row['post_title']).strip() if pd.notna(row['post_title'])\n",
    "\t\t\t\telse \"\"), \n",
    "\taxis=1\n",
    ").fillna(\"\")\n",
    "\n",
    "# Display first few rows and value counts of empty strings\n",
    "print(\"Sample texts:\")\n",
    "print(raw_data[\"text\"].head())\n",
    "print(\"\\nNumber of empty texts:\", (raw_data[\"text\"] == \"\").sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Posts and Comments:\n",
      "7963\n",
      "46349\n"
     ]
    }
   ],
   "source": [
    "# No.of Posts and Comments\n",
    "print(\"\\nNumber of Posts and Comments:\")\n",
    "print(raw_data[\"post_id\"].nunique())\n",
    "print(raw_data[\"comment_id\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = raw_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  S1: Filter to past 5 year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Posts and Comments after filtering:\n",
      "7963\n",
      "46349\n",
      "\n",
      "Date time range:\n",
      "2023-01-01 00:03:33\n",
      "2025-01-30 12:00:03\n"
     ]
    }
   ],
   "source": [
    "cutoff_date = datetime.now() - timedelta(days=5*365)\n",
    "\n",
    "filtered_data = filtered_data[filtered_data[\"readable_datetime\"] > cutoff_date]\n",
    "\n",
    "print(\"\\nNumber of Posts and Comments after filtering:\")\n",
    "print(filtered_data[\"post_id\"].nunique())\n",
    "print(filtered_data[\"comment_id\"].count())\n",
    "\n",
    "# Print date time range in the data\n",
    "print(\"\\nDate time range:\")\n",
    "print(filtered_data[\"readable_datetime\"].min())\n",
    "print(filtered_data[\"readable_datetime\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2: Text Length\n",
    "\n",
    "Short Texts: Extremely short texts (e.g., those with only one or two words) might not provide enough context and could be noise.\n",
    "\n",
    "Excessively Long Texts: Conversely, texts that far exceed the typical length for your domain might be off-topic or contain noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word count statistics:\n",
      "  count     mean      std    min    25%    50%    75%    max\n",
      "-------  -------  -------  -----  -----  -----  -----  -----\n",
      "  46349  164.151  314.498      3     32     70    169   6543\n",
      "\n",
      "\n",
      "Max words set to: 374\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "min_words = 3\n",
    "\n",
    "# Calculate word counts for each text\n",
    "word_counts = filtered_data['text'].str.split().str.len()\n",
    "\n",
    "print(\"\\nWord count statistics:\")\n",
    "print(tabulate([word_counts.describe()], headers='keys'))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Set max_words as the upper quartile (75th percentile) plus 1.5 times IQR\n",
    "Q3 = word_counts.quantile(0.75)\n",
    "Q1 = word_counts.quantile(0.25)\n",
    "IQR = Q3 - Q1\n",
    "max_words = int(Q3 + 1.5 * IQR)\n",
    "\n",
    "print(f\"Max words set to: {max_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Posts and Comments after filtering by word count:\n",
      "7305\n",
      "41745\n"
     ]
    }
   ],
   "source": [
    "# Filter out texts with less than min_words or more than max_words\n",
    "\n",
    "filtered_data = filtered_data[(word_counts >= min_words) & (word_counts <= max_words)]\n",
    "\n",
    "print(\"\\nNumber of Posts and Comments after filtering by word count:\")\n",
    "print(filtered_data[\"post_id\"].nunique())\n",
    "print(filtered_data[\"comment_id\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the filtered data in a new CSV file\n",
    "filtered_data.to_csv('filtered_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
